"""
Evaluation Metrics for Time Series Anomaly Detection
====================================================

This module provides comprehensive evaluation metrics specifically designed
for time series anomaly detection tasks.

Author: Fault Diagnosis Benchmark Team
Date: 2025-01-11
"""

import numpy as np
from sklearn.metrics import (
    precision_score, recall_score, f1_score, roc_auc_score,
    accuracy_score, confusion_matrix, precision_recall_curve
)
from typing import Dict, List, Tuple, Optional, Any
import logging
import warnings

from models.base_model import DataMetadata

logger = logging.getLogger(__name__)


class TimeSeriesEvaluator:
    """
    æ—¶åºå¼‚å¸¸æ£€æµ‹è¯„ä¼°å™¨
    
    æä¾›ä¸“é—¨é’ˆå¯¹æ—¶åºå¼‚å¸¸æ£€æµ‹çš„è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ï¼š
    - ä¼ ç»Ÿç‚¹çº§æŒ‡æ ‡ (precision, recall, f1)
    - æ—¶åºç‰¹å®šæŒ‡æ ‡ (point-adjusted metrics)
    - æ—©æœŸæ£€æµ‹æŒ‡æ ‡
    - å®¹å¿åº¦è¯„ä¼°
    """
    
    def __init__(self, tolerance: int = 0):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            tolerance: æ—¶é—´å®¹å¿åº¦ï¼Œç”¨äºå®¹å¿è¯„ä¼°
        """
        self.tolerance = tolerance
        self.supported_metrics = [
            'precision', 'recall', 'f1', 'accuracy', 'auc',
            'f1_point_adjusted', 'precision_pa', 'recall_pa',
            'detection_delay', 'early_detection_rate',
            'range_precision', 'range_recall', 'range_f1'
        ]
    
    def evaluate(self, y_true: np.ndarray, anomaly_scores: np.ndarray,
                metadata: DataMetadata, threshold: Optional[float] = None) -> Dict[str, float]:
        """
        æ ¹æ®å…ƒæ•°æ®è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„è¯„ä¼°æ–¹æ³•
        
        Args:
            y_true: çœŸå®æ ‡ç­¾
            anomaly_scores: å¼‚å¸¸åˆ†æ•°
            metadata: æ•°æ®å…ƒæ•°æ®
            threshold: å¼‚å¸¸æ£€æµ‹é˜ˆå€¼ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        
        logger.info(f"å¼€å§‹è¯„ä¼°ï¼Œæ•°æ®ç±»å‹: {metadata.fault_type}, "
                   f"æ ‡ç­¾ç²’åº¦: {metadata.label_granularity}")
        
        # æ ¹æ®æ ‡ç­¾ç²’åº¦å’Œæ•…éšœç±»å‹é€‰æ‹©è¯„ä¼°ç­–ç•¥
        if metadata.label_granularity == "point-wise":
            if metadata.fault_type == "binary":
                return self._evaluate_binary_pointwise(y_true, anomaly_scores, threshold)
            else:
                return self._evaluate_multiclass_pointwise(y_true, anomaly_scores, threshold)
        else:  # sequence-wise
            if metadata.fault_type == "binary":
                return self._evaluate_binary_sequence(y_true, anomaly_scores, threshold)
            else:
                return self._evaluate_multiclass_sequence(y_true, anomaly_scores, threshold)
    
    def _evaluate_binary_pointwise(self, y_true: np.ndarray, 
                                  anomaly_scores: np.ndarray,
                                  threshold: Optional[float] = None) -> Dict[str, float]:
        """äºŒåˆ†ç±»é€ç‚¹è¯„ä¼°"""
        
        # ç¡®å®šé˜ˆå€¼
        if threshold is None:
            threshold, _ = self._find_best_threshold(y_true, anomaly_scores)
        
        y_pred = (anomaly_scores > threshold).astype(int)
        
        # åŸºç¡€æŒ‡æ ‡
        results = {
            'threshold': threshold,
            'precision': precision_score(y_true, y_pred, zero_division=0),
            'recall': recall_score(y_true, y_pred, zero_division=0),
            'f1': f1_score(y_true, y_pred, zero_division=0),
            'accuracy': accuracy_score(y_true, y_pred),
        }
        
        # AUCï¼ˆå¦‚æœå¯èƒ½ï¼‰
        try:
            if len(np.unique(y_true)) > 1:
                results['auc'] = roc_auc_score(y_true, anomaly_scores)
            else:
                results['auc'] = np.nan
        except ValueError:
            results['auc'] = np.nan
        
        # Point-AdjustedæŒ‡æ ‡
        pa_metrics = self._compute_point_adjusted_metrics(y_true, y_pred)
        results.update(pa_metrics)
        
        # æ—¶åºç‰¹å®šæŒ‡æ ‡
        ts_metrics = self._compute_time_series_metrics(y_true, y_pred, anomaly_scores)
        results.update(ts_metrics)
        
        return results
    
    def _compute_point_adjusted_metrics(self, y_true: np.ndarray, 
                                      y_pred: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—Point-AdjustedæŒ‡æ ‡"""
        
        # æ‰¾åˆ°çœŸå®å’Œé¢„æµ‹çš„å¼‚å¸¸åŒºé—´
        true_ranges = self._find_anomaly_ranges(y_true)
        pred_ranges = self._find_anomaly_ranges(y_pred)
        
        if len(true_ranges) == 0 and len(pred_ranges) == 0:
            # æ²¡æœ‰å¼‚å¸¸
            return {
                'f1_point_adjusted': 1.0,
                'precision_pa': 1.0,
                'recall_pa': 1.0,
                'range_precision': 1.0,
                'range_recall': 1.0,
                'range_f1': 1.0
            }
        
        # è®¡ç®—åŒºé—´çº§åˆ«çš„TP, FP, FN
        tp_ranges = 0
        matched_true_ranges = set()
        
        for pred_range in pred_ranges:
            for i, true_range in enumerate(true_ranges):
                if self._ranges_overlap(pred_range, true_range):
                    tp_ranges += 1
                    matched_true_ranges.add(i)
                    break
        
        fp_ranges = len(pred_ranges) - tp_ranges
        fn_ranges = len(true_ranges) - len(matched_true_ranges)
        
        # Range-basedæŒ‡æ ‡
        range_precision = tp_ranges / max(len(pred_ranges), 1)
        range_recall = tp_ranges / max(len(true_ranges), 1)
        range_f1 = 2 * range_precision * range_recall / max(range_precision + range_recall, 1e-10)
        
        # Point-AdjustedæŒ‡æ ‡ï¼ˆæ›´ä¸¥æ ¼çš„ç‰ˆæœ¬ï¼‰
        if tp_ranges + fp_ranges == 0:
            precision_pa = 0.0
        else:
            precision_pa = tp_ranges / (tp_ranges + fp_ranges)
        
        if tp_ranges + fn_ranges == 0:
            recall_pa = 0.0 
        else:
            recall_pa = tp_ranges / (tp_ranges + fn_ranges)
        
        if precision_pa + recall_pa == 0:
            f1_pa = 0.0
        else:
            f1_pa = 2 * precision_pa * recall_pa / (precision_pa + recall_pa)
        
        return {
            'f1_point_adjusted': f1_pa,
            'precision_pa': precision_pa,
            'recall_pa': recall_pa,
            'range_precision': range_precision,
            'range_recall': range_recall,
            'range_f1': range_f1
        }
    
    def _compute_time_series_metrics(self, y_true: np.ndarray, y_pred: np.ndarray,
                                   anomaly_scores: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—æ—¶åºç‰¹å®šæŒ‡æ ‡"""
        
        true_ranges = self._find_anomaly_ranges(y_true)
        
        if len(true_ranges) == 0:
            return {
                'detection_delay': 0.0,
                'early_detection_rate': 1.0
            }
        
        # æ£€æµ‹å»¶è¿Ÿ
        total_delay = 0
        detected_ranges = 0
        early_detections = 0
        
        for true_start, true_end in true_ranges:
            # åœ¨çœŸå®å¼‚å¸¸åŒºé—´å†…æ‰¾åˆ°ç¬¬ä¸€ä¸ªé¢„æµ‹å¼‚å¸¸ç‚¹
            anomaly_region = y_pred[true_start:true_end+1]
            
            if np.any(anomaly_region):
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªé¢„æµ‹å¼‚å¸¸ç‚¹
                first_detection = np.argmax(anomaly_region) + true_start
                delay = max(0, first_detection - true_start)
                total_delay += delay
                detected_ranges += 1
                
                # æ—©æœŸæ£€æµ‹ï¼ˆåœ¨å¼‚å¸¸å¼€å§‹å‰å°±æ£€æµ‹åˆ°ï¼‰
                if first_detection <= true_start:
                    early_detections += 1
        
        avg_delay = total_delay / max(detected_ranges, 1)
        early_detection_rate = early_detections / len(true_ranges) if true_ranges else 0.0
        
        return {
            'detection_delay': avg_delay,
            'early_detection_rate': early_detection_rate
        }
    
    def _find_best_threshold(self, y_true: np.ndarray, 
                           anomaly_scores: np.ndarray) -> Tuple[float, float]:
        """æ‰¾åˆ°æœ€ä½³F1åˆ†æ•°å¯¹åº”çš„é˜ˆå€¼"""
        
        if len(np.unique(y_true)) < 2:
            # åªæœ‰ä¸€ä¸ªç±»åˆ«
            return np.percentile(anomaly_scores, 95), 0.0
        
        # ä½¿ç”¨precision-recallæ›²çº¿æ‰¾æœ€ä½³é˜ˆå€¼
        precision, recall, thresholds = precision_recall_curve(y_true, anomaly_scores)
        
        # è®¡ç®—F1åˆ†æ•°
        f1_scores = 2 * precision * recall / (precision + recall + 1e-10)
        
        # æ‰¾åˆ°æœ€ä½³F1å¯¹åº”çš„é˜ˆå€¼
        best_idx = np.argmax(f1_scores)
        best_f1 = f1_scores[best_idx]
        
        if best_idx < len(thresholds):
            best_threshold = thresholds[best_idx]
        else:
            best_threshold = np.percentile(anomaly_scores, 95)
        
        return best_threshold, best_f1
    
    def _find_anomaly_ranges(self, y: np.ndarray) -> List[Tuple[int, int]]:
        """æ‰¾åˆ°å¼‚å¸¸åŒºé—´"""
        ranges = []
        start = None
        
        for i, val in enumerate(y):
            if val == 1 and start is None:  # å¼‚å¸¸å¼€å§‹
                start = i
            elif val == 0 and start is not None:  # å¼‚å¸¸ç»“æŸ
                ranges.append((start, i-1))
                start = None
        
        # å¤„ç†åºåˆ—æœ«å°¾çš„å¼‚å¸¸
        if start is not None:
            ranges.append((start, len(y)-1))
        
        return ranges
    
    def _ranges_overlap(self, range1: Tuple[int, int], range2: Tuple[int, int]) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªåŒºé—´æ˜¯å¦é‡å """
        return max(range1[0], range2[0]) <= min(range1[1], range2[1])
    
    def _evaluate_multiclass_pointwise(self, y_true: np.ndarray, 
                                     anomaly_scores: np.ndarray,
                                     threshold: Optional[float] = None) -> Dict[str, float]:
        """å¤šç±»åˆ«é€ç‚¹è¯„ä¼°"""
        
        # å¯¹äºå¤šç±»åˆ«æƒ…å†µï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªåŸºç¡€å®ç°
        # å¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚æ‰©å±•
        
        logger.warning("å¤šç±»åˆ«ç‚¹çº§è¯„ä¼°å°šæœªå®Œå…¨å®ç°ï¼Œä½¿ç”¨åŸºç¡€æŒ‡æ ‡")
        
        if threshold is None:
            threshold = np.percentile(anomaly_scores, 95)
        
        # å°†å¤šç±»åˆ«é—®é¢˜ç®€åŒ–ä¸ºäºŒåˆ†ç±»ï¼ˆæ­£å¸¸ vs ä»»æ„å¼‚å¸¸ï¼‰
        y_binary = (y_true > 0).astype(int)
        y_pred = (anomaly_scores > threshold).astype(int)
        
        return {
            'threshold': threshold,
            'accuracy': accuracy_score(y_binary, y_pred),
            'precision': precision_score(y_binary, y_pred, zero_division=0),
            'recall': recall_score(y_binary, y_pred, zero_division=0),
            'f1': f1_score(y_binary, y_pred, zero_division=0)
        }
    
    def _evaluate_binary_sequence(self, y_true: np.ndarray, 
                                anomaly_scores: np.ndarray,
                                threshold: Optional[float] = None) -> Dict[str, float]:
        """äºŒåˆ†ç±»åºåˆ—çº§è¯„ä¼°"""
        
        logger.warning("åºåˆ—çº§è¯„ä¼°å°šæœªå®Œå…¨å®ç°ï¼Œä½¿ç”¨ç‚¹çº§è¯„ä¼°")
        return self._evaluate_binary_pointwise(y_true, anomaly_scores, threshold)
    
    def _evaluate_multiclass_sequence(self, y_true: np.ndarray, 
                                    anomaly_scores: np.ndarray,
                                    threshold: Optional[float] = None) -> Dict[str, float]:
        """å¤šç±»åˆ«åºåˆ—çº§è¯„ä¼°"""
        
        logger.warning("å¤šç±»åˆ«åºåˆ—çº§è¯„ä¼°å°šæœªå®Œå…¨å®ç°ï¼Œä½¿ç”¨åŸºç¡€æŒ‡æ ‡")
        return self._evaluate_multiclass_pointwise(y_true, anomaly_scores, threshold)
    
    def compute_confusion_matrix(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        """è®¡ç®—æ··æ·†çŸ©é˜µ"""
        return confusion_matrix(y_true, y_pred)
    
    def compute_roc_curve(self, y_true: np.ndarray, anomaly_scores: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """è®¡ç®—ROCæ›²çº¿"""
        from sklearn.metrics import roc_curve
        
        if len(np.unique(y_true)) < 2:
            logger.warning("åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œæ— æ³•è®¡ç®—ROCæ›²çº¿")
            return np.array([]), np.array([]), np.array([])
        
        return roc_curve(y_true, anomaly_scores)
    
    def compute_pr_curve(self, y_true: np.ndarray, anomaly_scores: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """è®¡ç®—Precision-Recallæ›²çº¿"""
        
        if len(np.unique(y_true)) < 2:
            logger.warning("åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œæ— æ³•è®¡ç®—PRæ›²çº¿")
            return np.array([]), np.array([]), np.array([])
        
        return precision_recall_curve(y_true, anomaly_scores)


class MetricsCalculator:
    """æŒ‡æ ‡è®¡ç®—å·¥å…·ç±»"""
    
    @staticmethod
    def calculate_all_metrics(y_true: np.ndarray, y_pred: np.ndarray, 
                            anomaly_scores: np.ndarray) -> Dict[str, Any]:
        """è®¡ç®—æ‰€æœ‰å¯èƒ½çš„æŒ‡æ ‡"""
        
        metrics = {}
        
        # åŸºç¡€åˆ†ç±»æŒ‡æ ‡
        metrics['accuracy'] = accuracy_score(y_true, y_pred)
        metrics['precision'] = precision_score(y_true, y_pred, zero_division=0)
        metrics['recall'] = recall_score(y_true, y_pred, zero_division=0)
        metrics['f1'] = f1_score(y_true, y_pred, zero_division=0)
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        metrics['confusion_matrix'] = cm.tolist()
        
        if cm.shape == (2, 2):
            tn, fp, fn, tp = cm.ravel()
            metrics['true_negatives'] = int(tn)
            metrics['false_positives'] = int(fp) 
            metrics['false_negatives'] = int(fn)
            metrics['true_positives'] = int(tp)
            
            # å…¶ä»–æŒ‡æ ‡
            metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0
            metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        
        # AUCæŒ‡æ ‡
        try:
            if len(np.unique(y_true)) > 1:
                metrics['auc'] = roc_auc_score(y_true, anomaly_scores)
            else:
                metrics['auc'] = np.nan
        except ValueError:
            metrics['auc'] = np.nan
        
        return metrics
    
    @staticmethod
    def print_metrics_report(metrics: Dict[str, Any], title: str = "è¯„ä¼°ç»“æœ") -> None:
        """æ‰“å°æ ¼å¼åŒ–çš„æŒ‡æ ‡æŠ¥å‘Š"""
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š {title}")
        print(f"{'='*60}")
        
        # åŸºç¡€æŒ‡æ ‡
        basic_metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']
        print("ğŸ“ˆ åŸºç¡€æŒ‡æ ‡:")
        for metric in basic_metrics:
            if metric in metrics:
                value = metrics[metric]
                if isinstance(value, float):
                    if np.isnan(value):
                        print(f"   - {metric}: N/A")
                    else:
                        print(f"   - {metric}: {value:.4f}")
                else:
                    print(f"   - {metric}: {value}")
        
        # Point-AdjustedæŒ‡æ ‡
        pa_metrics = ['f1_point_adjusted', 'precision_pa', 'recall_pa']
        pa_values = [metrics.get(m) for m in pa_metrics]
        if any(v is not None for v in pa_values):
            print("\nğŸ¯ Point-AdjustedæŒ‡æ ‡:")
            for metric in pa_metrics:
                if metric in metrics and metrics[metric] is not None:
                    print(f"   - {metric}: {metrics[metric]:.4f}")
        
        # æ—¶åºæŒ‡æ ‡
        ts_metrics = ['detection_delay', 'early_detection_rate']
        ts_values = [metrics.get(m) for m in ts_metrics]
        if any(v is not None for v in ts_values):
            print("\nâ° æ—¶åºæŒ‡æ ‡:")
            for metric in ts_metrics:
                if metric in metrics and metrics[metric] is not None:
                    print(f"   - {metric}: {metrics[metric]:.4f}")
        
        # é˜ˆå€¼ä¿¡æ¯
        if 'threshold' in metrics:
            print(f"\nğŸšï¸  æœ€ä½³é˜ˆå€¼: {metrics['threshold']:.4f}")
        
        print(f"{'='*60}")


# ä¾¿æ·å‡½æ•°
def evaluate_model(y_true: np.ndarray, anomaly_scores: np.ndarray, 
                   metadata: DataMetadata, threshold: Optional[float] = None) -> Dict[str, float]:
    """
    ä¾¿æ·çš„æ¨¡å‹è¯„ä¼°å‡½æ•°
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        anomaly_scores: å¼‚å¸¸åˆ†æ•°
        metadata: æ•°æ®å…ƒæ•°æ®
        threshold: æ£€æµ‹é˜ˆå€¼
        
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    evaluator = TimeSeriesEvaluator()
    return evaluator.evaluate(y_true, anomaly_scores, metadata, threshold)


def print_evaluation_report(results: Dict[str, float], title: str = "æ¨¡å‹è¯„ä¼°æŠ¥å‘Š") -> None:
    """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
    MetricsCalculator.print_metrics_report(results, title)
