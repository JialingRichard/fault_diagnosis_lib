{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6324fb6e",
   "metadata": {},
   "source": [
    "# 时序异常检测基准测试框架\n",
    "\n",
    "这个notebook是整个基准测试框架的核心运行脚本，用于:\n",
    "- 加载和管理实验配置\n",
    "- 数据集处理（未完成）\n",
    "- 执行完整的模型训练和评估流程\n",
    "- 生成结果报告和可视化（待添加新评估图像和指标）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca78605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入依赖库\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c486a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 放在任何 torch 操作之前（越靠前越好）\n",
    "import os\n",
    "\n",
    "# 1) 让 MIOpen 避免在第一次做耗时的穷举搜索（显著减少“第一次超久”）\n",
    "os.environ['MIOPEN_FIND_ENFORCE'] = '1'  # 只用已知解，跳过搜索\n",
    "# 可选：自定义/确保可写缓存目录，避免权限问题导致每次都重新搜索\n",
    "os.environ['MIOPEN_CACHE_DIR'] = os.path.expanduser('~/.cache/miopen')\n",
    "\n",
    "# 2) 可选：看日志定位问题时再启用\n",
    "# os.environ['MIOPEN_LOG_LEVEL'] = '5'\n",
    "\n",
    "# 3) 限定可见 GPU（如有多卡）\n",
    "# os.environ['HIP_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# 4) 小型 warmup，提前完成 HIP/MIOpen 初始化（放在创建模型前）\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    x = torch.randn(128, 128, device='cuda')\n",
    "    y = x @ x.t()\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e6ea3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.models.base_model:注册模型: isolation_forest\n",
      "INFO:src.models.base_model:注册模型: iforest\n",
      "INFO:src.models.base_model:注册模型: lstm_autoencoder\n",
      "INFO:src.models.base_model:注册模型: lstm_ae\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "项目根目录: /home/chen/dev/fault_diagnosis_lib/benchmark\n",
      "依赖库导入完成\n",
      "start\n",
      "ConfigManager 导入成功\n",
      "DataPipeline 导入成功\n",
      "ModelFactory 导入成功\n",
      "Trainer 导入成功\n",
      "Metrics 导入成功\n",
      "导入检测完成\n"
     ]
    }
   ],
   "source": [
    "# 添加项目路径\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"项目根目录: {project_root}\")\n",
    "print(\"依赖库导入完成\")\n",
    "\n",
    "# 分步导入框架组件\n",
    "try:\n",
    "    from src.config_manager import ConfigManager\n",
    "    print(\"ConfigManager 导入成功-error\")\n",
    "except ImportError as e:\n",
    "    # print(f\"ConfigManager 导入失败: {e}\")\n",
    "    print(\"start\")\n",
    "\n",
    "try:\n",
    "    from src.config_manager import ConfigManager\n",
    "    print(\"ConfigManager 导入成功\")\n",
    "except ImportError as e:\n",
    "    print(f\"ConfigManager 导入失败: {e}\")\n",
    "\n",
    "try:\n",
    "    from src.dataloaders import DataPipeline\n",
    "    print(\"DataPipeline 导入成功\")\n",
    "except ImportError as e:\n",
    "    print(f\"DataPipeline 导入失败: {e}\")\n",
    "\n",
    "try:\n",
    "    from src.models.base_model import ModelFactory\n",
    "    print(\"ModelFactory 导入成功\")\n",
    "except ImportError as e:\n",
    "    print(f\"ModelFactory 导入失败: {e}\")\n",
    "\n",
    "try:\n",
    "    from src.trainer import Trainer\n",
    "    print(\"Trainer 导入成功\")\n",
    "except ImportError as e:\n",
    "    print(f\"Trainer 导入失败: {e}\")\n",
    "\n",
    "try:\n",
    "    from src.metrics import TimeSeriesEvaluator, print_evaluation_report\n",
    "    print(\"Metrics 导入成功\")\n",
    "except ImportError as e:\n",
    "    print(f\"Metrics 导入失败: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# 设置随机种子\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"导入检测完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347131f",
   "metadata": {},
   "source": [
    "## 1. 配置管理与实验设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "208e1ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 创建配置管理器\n",
    "# # from src.config_manager import ConfigManager\n",
    "# config_manager = ConfigManager()\n",
    "\n",
    "# # 定义默认实验配置\n",
    "# default_config = {\n",
    "#     'experiment': {\n",
    "#         'name': 'benchmark_experiment_01',\n",
    "#         'description': '时序异常检测基准测试实验',\n",
    "#         'version': '1.0',\n",
    "#         'author': 'Benchmark Team',\n",
    "#         'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     },\n",
    "#     'data': {\n",
    "#         'dataset': 'swat',\n",
    "#         'train_file': '../data/raw/swat_train.csv',\n",
    "#         'test_file': '../data/raw/swat_test.csv',\n",
    "#         'preprocessing': {\n",
    "#             'normalize': True,\n",
    "#             'fill_missing': True,\n",
    "#             'remove_outliers': False\n",
    "#         },\n",
    "#         'window_size': 10,\n",
    "#         'stride': 1\n",
    "#     },\n",
    "#     'models': {\n",
    "#         'iforest': {\n",
    "#             'n_estimators': 100,\n",
    "#             'contamination': 0.1,\n",
    "#             'random_state': 42\n",
    "#         },\n",
    "#         'lstm_ae': {\n",
    "#             'hidden_dim': 64,\n",
    "#             'num_layers': 2,\n",
    "#             'dropout': 0.2,\n",
    "#             'lr': 0.001,\n",
    "#             'batch_size': 32,\n",
    "#             'epochs': 50,\n",
    "#             'patience': 10\n",
    "#         }\n",
    "#     },\n",
    "#     'evaluation': {\n",
    "#         'metrics': ['f1', 'precision', 'recall', 'auc', 'f1_point_adjusted'],\n",
    "#         'tolerance': 0,\n",
    "#         'auto_threshold': True\n",
    "#     },\n",
    "#     'output': {\n",
    "#         'save_results': True,\n",
    "#         'save_models': False,\n",
    "#         'generate_plots': True,\n",
    "#         'results_dir': '../results'\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # 保存默认配置\n",
    "# config_path = project_root / 'configs' / 'default_experiment.yaml'\n",
    "# config_manager.save_config(default_config, config_path)\n",
    "\n",
    "# print(\"默认配置已保存\")\n",
    "# print(f\"配置文件: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56150bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.config_manager:配置管理器初始化完成，配置目录: /home/chen/dev/fault_diagnosis_lib/benchmark/configs/default_experiment.yaml\n",
      "INFO:src.config_manager:配置文件加载成功: /home/chen/dev/fault_diagnosis_lib/benchmark/configs/default_experiment.yaml\n",
      "INFO:src.config_manager:配置验证通过\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实验配置:\n",
      "   - 实验名称: benchmark_experiment_01\n",
      "   - 数据集: swat\n",
      "   - 模型: ['iforest', 'lstm_ae']\n",
      "   - 评估指标: ['f1', 'precision', 'recall', 'auc', 'f1_point_adjusted']\n"
     ]
    }
   ],
   "source": [
    "# 加载用户的实验配置\n",
    "# config_file = project_root / 'configs' / 'default_experiment.yaml'\n",
    "config_file = \"/home/chen/dev/fault_diagnosis_lib/benchmark/configs/default_experiment.yaml\"\n",
    "config_manager = ConfigManager(config_dir = config_file)\n",
    "config = config_manager.load_config()\n",
    "\n",
    "print(\"实验配置:\")\n",
    "print(f\"   - 实验名称: {config['experiment']['name']}\")\n",
    "print(f\"   - 数据集: {config['data']['dataset']}\")\n",
    "print(f\"   - 模型: {list(config['models'].keys())}\")\n",
    "print(f\"   - 评估指标: {config['evaluation']['metrics']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a21c6ad",
   "metadata": {},
   "source": [
    "## 2. 数据加载与预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e64f4c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据集: swat\n",
      "真实数据加载失败: 'DataPipeline' object has no attribute 'load_dataset'\n",
      "使用模拟数据进行演示\n",
      "数据统计:\n",
      "   - 训练集: (1000, 10)\n",
      "   - 测试集: (1000, 10)\n",
      "   - 特征数: 10\n",
      "   - 故障类型: binary\n"
     ]
    }
   ],
   "source": [
    "# 创建数据管道\n",
    "data_pipeline = DataPipeline(config)\n",
    "\n",
    "# 加载数据\n",
    "dataset_name = config['data']['dataset']\n",
    "print(f\"加载数据集: {dataset_name}\")\n",
    "\n",
    "try:\n",
    "    # 尝试加载真实数据\n",
    "    train_data, train_labels, test_data, test_labels, metadata = data_pipeline.load_dataset(\n",
    "        dataset_name, \n",
    "        config['data']\n",
    "    )\n",
    "    print(\"数据加载成功\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"真实数据加载失败: {e}\")\n",
    "    print(\"使用模拟数据进行演示\")\n",
    "    \n",
    "    # 生成模拟数据\n",
    "    from src.dataloaders import DataMetadata\n",
    "    \n",
    "    # 创建模拟数据\n",
    "    n_samples = 1000\n",
    "    n_features = 10\n",
    "    \n",
    "    # 训练数据（大部分正常）\n",
    "    train_data = np.random.randn(n_samples, n_features)\n",
    "    train_labels = np.zeros(n_samples)\n",
    "    \n",
    "    # 测试数据（包含异常）\n",
    "    test_data = np.random.randn(n_samples, n_features)\n",
    "    test_labels = np.zeros(n_samples)\n",
    "    \n",
    "    # 注入异常\n",
    "    anomaly_indices = np.random.choice(n_samples, size=int(0.1 * n_samples), replace=False)\n",
    "    test_data[anomaly_indices] += 3  # 异常数据偏移\n",
    "    test_labels[anomaly_indices] = 1\n",
    "    # 创建元数据（使用正确的参数）\n",
    "    metadata = DataMetadata(\n",
    "        dataset_name=\"simulated_data\",\n",
    "        label_granularity=\"point-wise\",\n",
    "        fault_type=\"binary\", \n",
    "        num_classes=2,\n",
    "        sequence_length=n_samples,\n",
    "        feature_dim=n_features,\n",
    "    )\n",
    "\n",
    "print(f\"数据统计:\")\n",
    "print(f\"   - 训练集: {train_data.shape}\")\n",
    "print(f\"   - 测试集: {test_data.shape}\")\n",
    "print(f\"   - 特征数: {metadata.feature_dim}\")\n",
    "print(f\"   - 故障类型: {metadata.fault_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b1b9d",
   "metadata": {},
   "source": [
    "## 3. 模型训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f57b3408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始模型训练与评估\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 初始化结果存储\n",
    "experiment_results = {}\n",
    "model_objects = {}\n",
    "\n",
    "# 创建评估器\n",
    "evaluator = TimeSeriesEvaluator(tolerance=config['evaluation']['tolerance'])\n",
    "\n",
    "print(\"开始模型训练与评估\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e39c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.models.base_model:初始化模型: IsolationForest\n",
      "INFO:src.models.iforest:IsolationForest配置: {'contamination': 0.1, 'n_estimators': 100, 'max_samples': 'auto', 'max_features': 1.0, 'bootstrap': False, 'random_state': 42, 'n_jobs': -1, 'normalize': True}\n",
      "INFO:src.models.iforest:Isolation Forest模型构建完成，输入形状: (1000, 10)\n",
      "INFO:src.models.iforest:数据标准化器已拟合\n",
      "WARNING:src.models.iforest:没有找到正常样本标签，使用所有数据训练\n",
      "INFO:src.models.iforest:开始训练Isolation Forest...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nIsolation Forest\n",
      "----------------------------------------\n",
      "训练模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.models.iforest:Isolation Forest训练完成\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成预测...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.metrics:开始评估，数据类型: binary, 标签粒度: point-wise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评估性能...\n",
      "\n",
      "============================================================\n",
      "📊 Isolation Forest 评估结果\n",
      "============================================================\n",
      "📈 基础指标:\n",
      "   - accuracy: 0.9990\n",
      "   - precision: 1.0000\n",
      "   - recall: 0.9900\n",
      "   - f1: 0.9950\n",
      "   - auc: 1.0000\n",
      "\n",
      "🎯 Point-Adjusted指标:\n",
      "   - f1_point_adjusted: 0.9944\n",
      "   - precision_pa: 1.0000\n",
      "   - recall_pa: 0.9889\n",
      "\n",
      "⏰ 时序指标:\n",
      "   - detection_delay: 0.0000\n",
      "   - early_detection_rate: 0.9889\n",
      "\n",
      "🎚️  最佳阈值: 0.1614\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 训练和评估 Isolation Forest\n",
    "if 'iforest' in config['models']:\n",
    "    print(\"\\\\nIsolation Forest\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 创建模型\n",
    "    iforest_config = config['models']['iforest']\n",
    "    model = ModelFactory.create_model('iforest', iforest_config)\n",
    "    \n",
    "    # 训练\n",
    "    print(\"训练模型...\")\n",
    "    model.fit(train_data, metadata)\n",
    "    \n",
    "    # 预测 - 使用anomaly_scores而不是binary predictions\n",
    "    print(\"生成预测...\")\n",
    "    anomaly_scores = model.predict_anomaly_score(test_data)\n",
    "    \n",
    "    # 评估\n",
    "    print(\"评估性能...\")\n",
    "    results = evaluator.evaluate(test_labels, anomaly_scores, metadata)\n",
    "    \n",
    "    # 存储结果\n",
    "    experiment_results['iforest'] = results\n",
    "    model_objects['iforest'] = model\n",
    "    \n",
    "    # 显示结果\n",
    "    print_evaluation_report(results, \"Isolation Forest 评估结果\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88db918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.models.base_model:初始化模型: LSTM_AutoEncoder\n",
      "INFO:src.models.lstm_ae:使用设备: cuda\n",
      "INFO:src.models.lstm_ae:LSTM AutoEncoder配置: {'hidden_dim': 64, 'num_layers': 2, 'dropout': 0.2, 'bidirectional': False, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 50, 'patience': 10, 'normalize': True, 'sequence_length': 50, 'device': 'cuda', 'lr': 0.001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSTM AutoEncoder\n",
      "----------------------------------------\n",
      "训练模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.models.lstm_ae:LSTM AutoEncoder模型构建完成\n",
      "INFO:src.models.lstm_ae:模型参数数量: 119946\n",
      "INFO:src.models.lstm_ae:数据标准化器已拟合\n",
      "INFO:src.models.lstm_ae:使用 951 个正常序列训练模型\n",
      "INFO:src.models.lstm_ae:开始训练，设备: cuda\n"
     ]
    }
   ],
   "source": [
    "# 训练和评估 LSTM AutoEncoder\n",
    "if 'lstm_ae' in config['models']:\n",
    "    print(\"\\nLSTM AutoEncoder\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 创建模型\n",
    "    lstm_config = config['models']['lstm_ae']\n",
    "    model = ModelFactory.create_model('lstm_ae', lstm_config)\n",
    "    \n",
    "    # 训练\n",
    "    print(\"训练模型...\")\n",
    "    model.fit(train_data, train_labels)\n",
    "    \n",
    "    # 预测\n",
    "    print(\"生成预测...\")\n",
    "    anomaly_scores = model.predict(test_data, metadata)\n",
    "    \n",
    "    # 评估\n",
    "    print(\"评估性能...\")\n",
    "    results = evaluator.evaluate(test_labels, anomaly_scores, metadata)\n",
    "    \n",
    "    # 存储结果\n",
    "    experiment_results['lstm_ae'] = results\n",
    "    model_objects['lstm_ae'] = model\n",
    "    \n",
    "    # 显示结果\n",
    "    print_evaluation_report(results, \"LSTM AutoEncoder 评估结果\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb5a977",
   "metadata": {},
   "source": [
    "## 4. 结果对比与可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64496442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结果汇总对比\n",
    "if experiment_results:\n",
    "    print(\"\\n模型性能对比\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 创建对比表格\n",
    "    comparison_metrics = ['f1', 'precision', 'recall', 'auc', 'f1_point_adjusted']\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, results in experiment_results.items():\n",
    "        row = {'模型': model_name}\n",
    "        for metric in comparison_metrics:\n",
    "            if metric in results:\n",
    "                value = results[metric]\n",
    "                if isinstance(value, float) and not np.isnan(value):\n",
    "                    row[metric] = f\"{value:.4f}\"\n",
    "                else:\n",
    "                    row[metric] = \"N/A\"\n",
    "            else:\n",
    "                row[metric] = \"N/A\"\n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    # 显示对比表格\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # 找出最佳模型\n",
    "    best_model = None\n",
    "    best_f1 = -1\n",
    "    \n",
    "    for model_name, results in experiment_results.items():\n",
    "        if 'f1' in results and not np.isnan(results['f1']):\n",
    "            if results['f1'] > best_f1:\n",
    "                best_f1 = results['f1']\n",
    "                best_model = model_name\n",
    "    \n",
    "    if best_model:\n",
    "        print(f\"\\n最佳模型: {best_model} (F1 Score: {best_f1:.4f})\")\n",
    "else:\n",
    "    print(\"没有成功的实验结果\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化结果\n",
    "if experiment_results and config['output']['generate_plots']:\n",
    "    print(\"\\nGenerating visualization charts\")\n",
    "    \n",
    "    # Set plot style\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Time Series Anomaly Detection Benchmark Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Performance metrics bar chart\n",
    "    ax1 = axes[0, 0]\n",
    "    metrics_to_plot = ['f1', 'precision', 'recall']\n",
    "    model_names = list(experiment_results.keys())\n",
    "    \n",
    "    x = np.arange(len(metrics_to_plot))\n",
    "    width = 0.35 if len(model_names) == 2 else 0.25\n",
    "    \n",
    "    for i, model_name in enumerate(model_names):\n",
    "        values = []\n",
    "        for metric in metrics_to_plot:\n",
    "            value = experiment_results[model_name].get(metric, 0)\n",
    "            if isinstance(value, float) and not np.isnan(value):\n",
    "                values.append(value)\n",
    "            else:\n",
    "                values.append(0)\n",
    "        \n",
    "        ax1.bar(x + i * width, values, width, label=model_name, alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('Metric')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_title('Performance Comparison')\n",
    "    ax1.set_xticks(x + width * (len(model_names) - 1) / 2)\n",
    "    ax1.set_xticklabels(metrics_to_plot)\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Anomaly score distribution (example for first model)\n",
    "   \n",
    "    \n",
    "    # 3. Time series anomaly detection example (first 200 points)\n",
    "    ax3 = axes[1, 0]\n",
    "    n_points = min(200, len(test_labels))\n",
    "    time_indices = np.arange(n_points)\n",
    "    \n",
    "    # Show data for the first feature\n",
    "    ax3.plot(time_indices, test_data[:n_points, 0], 'b-', alpha=0.7, label='Time Series')\n",
    "    \n",
    "    # Mark anomaly points\n",
    "    anomaly_indices = np.where(test_labels[:n_points] == 1)[0]\n",
    "    if len(anomaly_indices) > 0:\n",
    "        ax3.scatter(anomaly_indices, test_data[anomaly_indices, 0], \n",
    "                   color='red', s=50, label='True Anomaly', zorder=5)\n",
    "    \n",
    "    ax3.set_xlabel('Time Index')\n",
    "    ax3.set_ylabel('Feature Value')\n",
    "    ax3.set_title('Time Series Anomaly Detection Example')\n",
    "    ax3.legend()\n",
    "    ax3.grid(alpha=0.3)\n",
    "    \n",
    "    # 4. Confusion matrix (for best model)\n",
    "    if best_model:\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        import itertools\n",
    "        \n",
    "        ax4 = axes[1, 1]\n",
    "        best_results = experiment_results[best_model]\n",
    "        \n",
    "        # Get best model predictions (binarized)\n",
    "        best_model_obj = model_objects[best_model]\n",
    "        if best_model == 'iforest':\n",
    "            best_pred = best_model_obj.predict(test_data)\n",
    "        elif best_model == 'lstm_ae':\n",
    "            best_pred = (best_model_obj.predict(test_data, metadata) > np.percentile(best_model_obj.predict(train_data, metadata), 90)).astype(int)\n",
    "        else:\n",
    "            best_pred = np.zeros_like(test_labels)\n",
    "        \n",
    "        cm = confusion_matrix(test_labels, best_pred)\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax4)\n",
    "        ax4.set_xlabel('Predicted Label')\n",
    "        ax4.set_ylabel('True Label')\n",
    "        ax4.set_title(f'{best_model} Confusion Matrix')\n",
    "        ax4.xaxis.set_ticklabels(['Normal', 'Anomaly'])\n",
    "        ax4.yaxis.set_ticklabels(['Normal', 'Anomaly'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Visualization charts generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc99929",
   "metadata": {},
   "source": [
    "## 5. 结果保存与报告生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存实验结果\n",
    "if config['output']['save_results'] and experiment_results:\n",
    "    print(\"\\nSaving experiment results\")\n",
    "    \n",
    "    # Create results directory\n",
    "    results_dir = project_root / config['output']['results_dir']\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Generate timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    experiment_name = config['experiment']['name']\n",
    "    \n",
    "    # 保存详细结果，修正元数据字段\n",
    "    train_size = train_data.shape[0] if 'train_data' in locals() else None\n",
    "    test_size = test_data.shape[0] if 'test_data' in locals() else None\n",
    "    anomaly_rate = (np.sum(test_labels) / len(test_labels)) if 'test_labels' in locals() else None\n",
    "    detailed_results = {\n",
    "        'experiment_info': config['experiment'],\n",
    "        'data_info': {\n",
    "            'dataset': getattr(metadata, 'dataset_name', 'unknown'),\n",
    "            'n_features': getattr(metadata, 'feature_dim', None),\n",
    "            'train_size': train_size,\n",
    "            'test_size': test_size,\n",
    "            'anomaly_rate': anomaly_rate,\n",
    "            'fault_type': getattr(metadata, 'fault_type', None)\n",
    "        },\n",
    "        'model_results': experiment_results,\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "print(\"结果保存功能待细化\")\n",
    "\n",
    "    # 保存为YAML文件\n",
    "    # results_file = results_dir / f\"{experiment_name}_{timestamp}.yaml\"\n",
    "    # with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    #     yaml.dump(detailed_results, f, default_flow_style=False, allow_unicode=True)\n",
    "    \n",
    "    # print(f\"Results saved to: {results_file}\")\n",
    "    \n",
    "    # # 保存对比表格为CSV\n",
    "    # if 'comparison_df' in locals():\n",
    "    #     csv_file = results_dir / f\"{experiment_name}_{timestamp}_comparison.csv\"\n",
    "    #     comparison_df.to_csv(csv_file, index=False)\n",
    "    #     print(f\"Comparison table saved to: {csv_file}\")\n",
    "    \n",
    "    # # 保存可视化图表\n",
    "    # if config['output']['generate_plots']:\n",
    "    #     plot_file = results_dir / f\"{experiment_name}_{timestamp}_plots.png\"\n",
    "    #     if 'fig' in locals():\n",
    "    #         fig.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "    #         print(f\"Plot saved to: {plot_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb395a",
   "metadata": {},
   "source": [
    "## 6. 实验总结与建议"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb53ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成实验总结报告\n",
    "print(\"\\n实验总结报告\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"实验名称: {config['experiment']['name']}\")\n",
    "print(f\"数据集: {metadata.dataset_name}\")\n",
    "\n",
    "print(f\"评估指标: {', '.join(config['evaluation']['metrics'])}\")\n",
    "\n",
    "if experiment_results:\n",
    "    print(f\"\\n主要发现:\")\n",
    "    \n",
    "    # 分析各模型性能\n",
    "    for model_name, results in experiment_results.items():\n",
    "        f1_score = results.get('f1', 0)\n",
    "        precision = results.get('precision', 0)\n",
    "        recall = results.get('recall', 0)\n",
    "        \n",
    "        print(f\" - {model_name}: F1={f1_score:.4f}, Precision={precision:.4f}, Recall={recall:.4f}\")\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "print(\"\\n基准测试实验完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1523ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量实验示例（可选运行）\n",
    "def run_batch_experiments(base_config, param_grid):\n",
    "    \"\"\"\n",
    "    运行批量实验进行参数扫描\n",
    "    \n",
    "    Args:\n",
    "        base_config: 基础配置\n",
    "        param_grid: 参数网格\n",
    "    \"\"\"\n",
    "    batch_results = []\n",
    "    \n",
    "    for i, params in enumerate(param_grid):\n",
    "        print(f\"\\n 批量实验 {i+1}/{len(param_grid)}\")\n",
    "        \n",
    "        # 更新配置\n",
    "        current_config = base_config.copy()\n",
    "        for key, value in params.items():\n",
    "            # 支持嵌套键如 'models.iforest.n_estimators'\n",
    "            keys = key.split('.')\n",
    "            target = current_config\n",
    "            for k in keys[:-1]:\n",
    "                target = target[k]\n",
    "            target[keys[-1]] = value\n",
    "        \n",
    "        # 运行实验（这里简化为示例）\n",
    "        print(f\"   参数: {params}\")\n",
    "        \n",
    "        # TODO: 在这里添加完整的实验流程\n",
    "        # 这需要重构上面的代码为函数形式\n",
    "        \n",
    "        batch_results.append({\n",
    "            'params': params,\n",
    "            'results': {}  # 实际结果\n",
    "        })\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "# 示例参数网格（实际使用时取消注释）\n",
    "# param_grid = [\n",
    "#     {'models.iforest.n_estimators': 50, 'models.iforest.contamination': 0.05},\n",
    "#     {'models.iforest.n_estimators': 100, 'models.iforest.contamination': 0.1},\n",
    "#     {'models.iforest.n_estimators': 200, 'models.iforest.contamination': 0.15},\n",
    "# ]\n",
    "\n",
    "# # 运行批量实验\n",
    "# batch_results = run_batch_experiments(config, param_grid)\n",
    "\n",
    "print(\"批量实验功能待定义\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_py311_rocm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
