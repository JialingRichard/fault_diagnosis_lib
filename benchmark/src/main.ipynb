{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6324fb6e",
   "metadata": {},
   "source": [
    "# æ—¶åºå¼‚å¸¸æ£€æµ‹åŸºå‡†æµ‹è¯•æ¡†æ¶\n",
    "\n",
    "è¿™ä¸ªnotebookæ˜¯æ•´ä¸ªåŸºå‡†æµ‹è¯•æ¡†æ¶çš„æ ¸å¿ƒè¿è¡Œè„šæœ¬ï¼Œç”¨äº:\n",
    "- åŠ è½½å’Œç®¡ç†å®éªŒé…ç½®\n",
    "- æ•°æ®é›†å¤„ç†ï¼ˆæœªå®Œæˆï¼‰\n",
    "- æ‰§è¡Œå®Œæ•´çš„æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°æµç¨‹\n",
    "- ç”Ÿæˆç»“æœæŠ¥å‘Šå’Œå¯è§†åŒ–ï¼ˆå¾…æ·»åŠ æ–°è¯„ä¼°å›¾åƒå’ŒæŒ‡æ ‡ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca78605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥ä¾èµ–åº“\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c486a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ”¾åœ¨ä»»ä½• torch æ“ä½œä¹‹å‰ï¼ˆè¶Šé å‰è¶Šå¥½ï¼‰\n",
    "import os\n",
    "\n",
    "# 1) è®© MIOpen é¿å…åœ¨ç¬¬ä¸€æ¬¡åšè€—æ—¶çš„ç©·ä¸¾æœç´¢ï¼ˆæ˜¾è‘—å‡å°‘â€œç¬¬ä¸€æ¬¡è¶…ä¹…â€ï¼‰\n",
    "os.environ['MIOPEN_FIND_ENFORCE'] = '1'  # åªç”¨å·²çŸ¥è§£ï¼Œè·³è¿‡æœç´¢\n",
    "# å¯é€‰ï¼šè‡ªå®šä¹‰/ç¡®ä¿å¯å†™ç¼“å­˜ç›®å½•ï¼Œé¿å…æƒé™é—®é¢˜å¯¼è‡´æ¯æ¬¡éƒ½é‡æ–°æœç´¢\n",
    "os.environ['MIOPEN_CACHE_DIR'] = os.path.expanduser('~/.cache/miopen')\n",
    "\n",
    "# 2) å¯é€‰ï¼šçœ‹æ—¥å¿—å®šä½é—®é¢˜æ—¶å†å¯ç”¨\n",
    "# os.environ['MIOPEN_LOG_LEVEL'] = '5'\n",
    "\n",
    "# 3) é™å®šå¯è§ GPUï¼ˆå¦‚æœ‰å¤šå¡ï¼‰\n",
    "# os.environ['HIP_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# 4) å°å‹ warmupï¼Œæå‰å®Œæˆ HIP/MIOpen åˆå§‹åŒ–ï¼ˆæ”¾åœ¨åˆ›å»ºæ¨¡å‹å‰ï¼‰\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    x = torch.randn(128, 128, device='cuda')\n",
    "    y = x @ x.t()\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e6ea3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.models.base_model:æ³¨å†Œæ¨¡å‹: isolation_forest\n",
      "INFO:src.models.base_model:æ³¨å†Œæ¨¡å‹: iforest\n",
      "INFO:src.models.base_model:æ³¨å†Œæ¨¡å‹: lstm_autoencoder\n",
      "INFO:src.models.base_model:æ³¨å†Œæ¨¡å‹: lstm_ae\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¡¹ç›®æ ¹ç›®å½•: /home/chen/dev/fault_diagnosis_lib/benchmark\n",
      "ä¾èµ–åº“å¯¼å…¥å®Œæˆ\n",
      "start\n",
      "ConfigManager å¯¼å…¥æˆåŠŸ\n",
      "DataPipeline å¯¼å…¥æˆåŠŸ\n",
      "ModelFactory å¯¼å…¥æˆåŠŸ\n",
      "Trainer å¯¼å…¥æˆåŠŸ\n",
      "Metrics å¯¼å…¥æˆåŠŸ\n",
      "å¯¼å…¥æ£€æµ‹å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# æ·»åŠ é¡¹ç›®è·¯å¾„\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"é¡¹ç›®æ ¹ç›®å½•: {project_root}\")\n",
    "print(\"ä¾èµ–åº“å¯¼å…¥å®Œæˆ\")\n",
    "\n",
    "# åˆ†æ­¥å¯¼å…¥æ¡†æ¶ç»„ä»¶\n",
    "try:\n",
    "    from src.config_manager import ConfigManager\n",
    "    print(\"ConfigManager å¯¼å…¥æˆåŠŸ-error\")\n",
    "except ImportError as e:\n",
    "    # print(f\"ConfigManager å¯¼å…¥å¤±è´¥: {e}\")\n",
    "    print(\"start\")\n",
    "\n",
    "try:\n",
    "    from src.config_manager import ConfigManager\n",
    "    print(\"ConfigManager å¯¼å…¥æˆåŠŸ\")\n",
    "except ImportError as e:\n",
    "    print(f\"ConfigManager å¯¼å…¥å¤±è´¥: {e}\")\n",
    "\n",
    "try:\n",
    "    from src.dataloaders import DataPipeline\n",
    "    print(\"DataPipeline å¯¼å…¥æˆåŠŸ\")\n",
    "except ImportError as e:\n",
    "    print(f\"DataPipeline å¯¼å…¥å¤±è´¥: {e}\")\n",
    "\n",
    "try:\n",
    "    from src.models.base_model import ModelFactory\n",
    "    print(\"ModelFactory å¯¼å…¥æˆåŠŸ\")\n",
    "except ImportError as e:\n",
    "    print(f\"ModelFactory å¯¼å…¥å¤±è´¥: {e}\")\n",
    "\n",
    "try:\n",
    "    from src.trainer import Trainer\n",
    "    print(\"Trainer å¯¼å…¥æˆåŠŸ\")\n",
    "except ImportError as e:\n",
    "    print(f\"Trainer å¯¼å…¥å¤±è´¥: {e}\")\n",
    "\n",
    "try:\n",
    "    from src.metrics import TimeSeriesEvaluator, print_evaluation_report\n",
    "    print(\"Metrics å¯¼å…¥æˆåŠŸ\")\n",
    "except ImportError as e:\n",
    "    print(f\"Metrics å¯¼å…¥å¤±è´¥: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"å¯¼å…¥æ£€æµ‹å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347131f",
   "metadata": {},
   "source": [
    "## 1. é…ç½®ç®¡ç†ä¸å®éªŒè®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "208e1ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # åˆ›å»ºé…ç½®ç®¡ç†å™¨\n",
    "# # from src.config_manager import ConfigManager\n",
    "# config_manager = ConfigManager()\n",
    "\n",
    "# # å®šä¹‰é»˜è®¤å®éªŒé…ç½®\n",
    "# default_config = {\n",
    "#     'experiment': {\n",
    "#         'name': 'benchmark_experiment_01',\n",
    "#         'description': 'æ—¶åºå¼‚å¸¸æ£€æµ‹åŸºå‡†æµ‹è¯•å®éªŒ',\n",
    "#         'version': '1.0',\n",
    "#         'author': 'Benchmark Team',\n",
    "#         'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     },\n",
    "#     'data': {\n",
    "#         'dataset': 'swat',\n",
    "#         'train_file': '../data/raw/swat_train.csv',\n",
    "#         'test_file': '../data/raw/swat_test.csv',\n",
    "#         'preprocessing': {\n",
    "#             'normalize': True,\n",
    "#             'fill_missing': True,\n",
    "#             'remove_outliers': False\n",
    "#         },\n",
    "#         'window_size': 10,\n",
    "#         'stride': 1\n",
    "#     },\n",
    "#     'models': {\n",
    "#         'iforest': {\n",
    "#             'n_estimators': 100,\n",
    "#             'contamination': 0.1,\n",
    "#             'random_state': 42\n",
    "#         },\n",
    "#         'lstm_ae': {\n",
    "#             'hidden_dim': 64,\n",
    "#             'num_layers': 2,\n",
    "#             'dropout': 0.2,\n",
    "#             'lr': 0.001,\n",
    "#             'batch_size': 32,\n",
    "#             'epochs': 50,\n",
    "#             'patience': 10\n",
    "#         }\n",
    "#     },\n",
    "#     'evaluation': {\n",
    "#         'metrics': ['f1', 'precision', 'recall', 'auc', 'f1_point_adjusted'],\n",
    "#         'tolerance': 0,\n",
    "#         'auto_threshold': True\n",
    "#     },\n",
    "#     'output': {\n",
    "#         'save_results': True,\n",
    "#         'save_models': False,\n",
    "#         'generate_plots': True,\n",
    "#         'results_dir': '../results'\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # ä¿å­˜é»˜è®¤é…ç½®\n",
    "# config_path = project_root / 'configs' / 'default_experiment.yaml'\n",
    "# config_manager.save_config(default_config, config_path)\n",
    "\n",
    "# print(\"é»˜è®¤é…ç½®å·²ä¿å­˜\")\n",
    "# print(f\"é…ç½®æ–‡ä»¶: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56150bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.config_manager:é…ç½®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œé…ç½®ç›®å½•: /home/chen/dev/fault_diagnosis_lib/benchmark/configs/default_experiment.yaml\n",
      "INFO:src.config_manager:é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: /home/chen/dev/fault_diagnosis_lib/benchmark/configs/default_experiment.yaml\n",
      "INFO:src.config_manager:é…ç½®éªŒè¯é€šè¿‡\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å®éªŒé…ç½®:\n",
      "   - å®éªŒåç§°: benchmark_experiment_01\n",
      "   - æ•°æ®é›†: swat\n",
      "   - æ¨¡å‹: ['iforest', 'lstm_ae']\n",
      "   - è¯„ä¼°æŒ‡æ ‡: ['f1', 'precision', 'recall', 'auc', 'f1_point_adjusted']\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½ç”¨æˆ·çš„å®éªŒé…ç½®\n",
    "# config_file = project_root / 'configs' / 'default_experiment.yaml'\n",
    "config_file = \"/home/chen/dev/fault_diagnosis_lib/benchmark/configs/default_experiment.yaml\"\n",
    "config_manager = ConfigManager(config_dir = config_file)\n",
    "config = config_manager.load_config()\n",
    "\n",
    "print(\"å®éªŒé…ç½®:\")\n",
    "print(f\"   - å®éªŒåç§°: {config['experiment']['name']}\")\n",
    "print(f\"   - æ•°æ®é›†: {config['data']['dataset']}\")\n",
    "print(f\"   - æ¨¡å‹: {list(config['models'].keys())}\")\n",
    "print(f\"   - è¯„ä¼°æŒ‡æ ‡: {config['evaluation']['metrics']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a21c6ad",
   "metadata": {},
   "source": [
    "## 2. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e64f4c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½æ•°æ®é›†: swat\n",
      "çœŸå®æ•°æ®åŠ è½½å¤±è´¥: 'DataPipeline' object has no attribute 'load_dataset'\n",
      "ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º\n",
      "æ•°æ®ç»Ÿè®¡:\n",
      "   - è®­ç»ƒé›†: (1000, 10)\n",
      "   - æµ‹è¯•é›†: (1000, 10)\n",
      "   - ç‰¹å¾æ•°: 10\n",
      "   - æ•…éšœç±»å‹: binary\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºæ•°æ®ç®¡é“\n",
    "data_pipeline = DataPipeline(config)\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "dataset_name = config['data']['dataset']\n",
    "print(f\"åŠ è½½æ•°æ®é›†: {dataset_name}\")\n",
    "\n",
    "try:\n",
    "    # å°è¯•åŠ è½½çœŸå®æ•°æ®\n",
    "    train_data, train_labels, test_data, test_labels, metadata = data_pipeline.load_dataset(\n",
    "        dataset_name, \n",
    "        config['data']\n",
    "    )\n",
    "    print(\"æ•°æ®åŠ è½½æˆåŠŸ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"çœŸå®æ•°æ®åŠ è½½å¤±è´¥: {e}\")\n",
    "    print(\"ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º\")\n",
    "    \n",
    "    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®\n",
    "    from src.dataloaders import DataMetadata\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®\n",
    "    n_samples = 1000\n",
    "    n_features = 10\n",
    "    \n",
    "    # è®­ç»ƒæ•°æ®ï¼ˆå¤§éƒ¨åˆ†æ­£å¸¸ï¼‰\n",
    "    train_data = np.random.randn(n_samples, n_features)\n",
    "    train_labels = np.zeros(n_samples)\n",
    "    \n",
    "    # æµ‹è¯•æ•°æ®ï¼ˆåŒ…å«å¼‚å¸¸ï¼‰\n",
    "    test_data = np.random.randn(n_samples, n_features)\n",
    "    test_labels = np.zeros(n_samples)\n",
    "    \n",
    "    # æ³¨å…¥å¼‚å¸¸\n",
    "    anomaly_indices = np.random.choice(n_samples, size=int(0.1 * n_samples), replace=False)\n",
    "    test_data[anomaly_indices] += 3  # å¼‚å¸¸æ•°æ®åç§»\n",
    "    test_labels[anomaly_indices] = 1\n",
    "    # åˆ›å»ºå…ƒæ•°æ®ï¼ˆä½¿ç”¨æ­£ç¡®çš„å‚æ•°ï¼‰\n",
    "    metadata = DataMetadata(\n",
    "        dataset_name=\"simulated_data\",\n",
    "        label_granularity=\"point-wise\",\n",
    "        fault_type=\"binary\", \n",
    "        num_classes=2,\n",
    "        sequence_length=n_samples,\n",
    "        feature_dim=n_features,\n",
    "    )\n",
    "\n",
    "print(f\"æ•°æ®ç»Ÿè®¡:\")\n",
    "print(f\"   - è®­ç»ƒé›†: {train_data.shape}\")\n",
    "print(f\"   - æµ‹è¯•é›†: {test_data.shape}\")\n",
    "print(f\"   - ç‰¹å¾æ•°: {metadata.feature_dim}\")\n",
    "print(f\"   - æ•…éšœç±»å‹: {metadata.fault_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b1b9d",
   "metadata": {},
   "source": [
    "## 3. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f57b3408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–ç»“æœå­˜å‚¨\n",
    "experiment_results = {}\n",
    "model_objects = {}\n",
    "\n",
    "# åˆ›å»ºè¯„ä¼°å™¨\n",
    "evaluator = TimeSeriesEvaluator(tolerance=config['evaluation']['tolerance'])\n",
    "\n",
    "print(\"å¼€å§‹æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e39c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.models.base_model:åˆå§‹åŒ–æ¨¡å‹: IsolationForest\n",
      "INFO:src.models.iforest:IsolationForesté…ç½®: {'contamination': 0.1, 'n_estimators': 100, 'max_samples': 'auto', 'max_features': 1.0, 'bootstrap': False, 'random_state': 42, 'n_jobs': -1, 'normalize': True}\n",
      "INFO:src.models.iforest:Isolation Forestæ¨¡å‹æ„å»ºå®Œæˆï¼Œè¾“å…¥å½¢çŠ¶: (1000, 10)\n",
      "INFO:src.models.iforest:æ•°æ®æ ‡å‡†åŒ–å™¨å·²æ‹Ÿåˆ\n",
      "WARNING:src.models.iforest:æ²¡æœ‰æ‰¾åˆ°æ­£å¸¸æ ·æœ¬æ ‡ç­¾ï¼Œä½¿ç”¨æ‰€æœ‰æ•°æ®è®­ç»ƒ\n",
      "INFO:src.models.iforest:å¼€å§‹è®­ç»ƒIsolation Forest...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nIsolation Forest\n",
      "----------------------------------------\n",
      "è®­ç»ƒæ¨¡å‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.models.iforest:Isolation Forestè®­ç»ƒå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç”Ÿæˆé¢„æµ‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.metrics:å¼€å§‹è¯„ä¼°ï¼Œæ•°æ®ç±»å‹: binary, æ ‡ç­¾ç²’åº¦: point-wise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¯„ä¼°æ€§èƒ½...\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š Isolation Forest è¯„ä¼°ç»“æœ\n",
      "============================================================\n",
      "ğŸ“ˆ åŸºç¡€æŒ‡æ ‡:\n",
      "   - accuracy: 0.9990\n",
      "   - precision: 1.0000\n",
      "   - recall: 0.9900\n",
      "   - f1: 0.9950\n",
      "   - auc: 1.0000\n",
      "\n",
      "ğŸ¯ Point-AdjustedæŒ‡æ ‡:\n",
      "   - f1_point_adjusted: 0.9944\n",
      "   - precision_pa: 1.0000\n",
      "   - recall_pa: 0.9889\n",
      "\n",
      "â° æ—¶åºæŒ‡æ ‡:\n",
      "   - detection_delay: 0.0000\n",
      "   - early_detection_rate: 0.9889\n",
      "\n",
      "ğŸšï¸  æœ€ä½³é˜ˆå€¼: 0.1614\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒå’Œè¯„ä¼° Isolation Forest\n",
    "if 'iforest' in config['models']:\n",
    "    print(\"\\\\nIsolation Forest\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡å‹\n",
    "    iforest_config = config['models']['iforest']\n",
    "    model = ModelFactory.create_model('iforest', iforest_config)\n",
    "    \n",
    "    # è®­ç»ƒ\n",
    "    print(\"è®­ç»ƒæ¨¡å‹...\")\n",
    "    model.fit(train_data, metadata)\n",
    "    \n",
    "    # é¢„æµ‹ - ä½¿ç”¨anomaly_scoresè€Œä¸æ˜¯binary predictions\n",
    "    print(\"ç”Ÿæˆé¢„æµ‹...\")\n",
    "    anomaly_scores = model.predict_anomaly_score(test_data)\n",
    "    \n",
    "    # è¯„ä¼°\n",
    "    print(\"è¯„ä¼°æ€§èƒ½...\")\n",
    "    results = evaluator.evaluate(test_labels, anomaly_scores, metadata)\n",
    "    \n",
    "    # å­˜å‚¨ç»“æœ\n",
    "    experiment_results['iforest'] = results\n",
    "    model_objects['iforest'] = model\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    print_evaluation_report(results, \"Isolation Forest è¯„ä¼°ç»“æœ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88db918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.models.base_model:åˆå§‹åŒ–æ¨¡å‹: LSTM_AutoEncoder\n",
      "INFO:src.models.lstm_ae:ä½¿ç”¨è®¾å¤‡: cuda\n",
      "INFO:src.models.lstm_ae:LSTM AutoEncoderé…ç½®: {'hidden_dim': 64, 'num_layers': 2, 'dropout': 0.2, 'bidirectional': False, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 50, 'patience': 10, 'normalize': True, 'sequence_length': 50, 'device': 'cuda', 'lr': 0.001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSTM AutoEncoder\n",
      "----------------------------------------\n",
      "è®­ç»ƒæ¨¡å‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.models.lstm_ae:LSTM AutoEncoderæ¨¡å‹æ„å»ºå®Œæˆ\n",
      "INFO:src.models.lstm_ae:æ¨¡å‹å‚æ•°æ•°é‡: 119946\n",
      "INFO:src.models.lstm_ae:æ•°æ®æ ‡å‡†åŒ–å™¨å·²æ‹Ÿåˆ\n",
      "INFO:src.models.lstm_ae:ä½¿ç”¨ 951 ä¸ªæ­£å¸¸åºåˆ—è®­ç»ƒæ¨¡å‹\n",
      "INFO:src.models.lstm_ae:å¼€å§‹è®­ç»ƒï¼Œè®¾å¤‡: cuda\n"
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒå’Œè¯„ä¼° LSTM AutoEncoder\n",
    "if 'lstm_ae' in config['models']:\n",
    "    print(\"\\nLSTM AutoEncoder\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡å‹\n",
    "    lstm_config = config['models']['lstm_ae']\n",
    "    model = ModelFactory.create_model('lstm_ae', lstm_config)\n",
    "    \n",
    "    # è®­ç»ƒ\n",
    "    print(\"è®­ç»ƒæ¨¡å‹...\")\n",
    "    model.fit(train_data, train_labels)\n",
    "    \n",
    "    # é¢„æµ‹\n",
    "    print(\"ç”Ÿæˆé¢„æµ‹...\")\n",
    "    anomaly_scores = model.predict(test_data, metadata)\n",
    "    \n",
    "    # è¯„ä¼°\n",
    "    print(\"è¯„ä¼°æ€§èƒ½...\")\n",
    "    results = evaluator.evaluate(test_labels, anomaly_scores, metadata)\n",
    "    \n",
    "    # å­˜å‚¨ç»“æœ\n",
    "    experiment_results['lstm_ae'] = results\n",
    "    model_objects['lstm_ae'] = model\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    print_evaluation_report(results, \"LSTM AutoEncoder è¯„ä¼°ç»“æœ\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb5a977",
   "metadata": {},
   "source": [
    "## 4. ç»“æœå¯¹æ¯”ä¸å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64496442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»“æœæ±‡æ€»å¯¹æ¯”\n",
    "if experiment_results:\n",
    "    print(\"\\næ¨¡å‹æ€§èƒ½å¯¹æ¯”\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼\n",
    "    comparison_metrics = ['f1', 'precision', 'recall', 'auc', 'f1_point_adjusted']\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, results in experiment_results.items():\n",
    "        row = {'æ¨¡å‹': model_name}\n",
    "        for metric in comparison_metrics:\n",
    "            if metric in results:\n",
    "                value = results[metric]\n",
    "                if isinstance(value, float) and not np.isnan(value):\n",
    "                    row[metric] = f\"{value:.4f}\"\n",
    "                else:\n",
    "                    row[metric] = \"N/A\"\n",
    "            else:\n",
    "                row[metric] = \"N/A\"\n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    # æ˜¾ç¤ºå¯¹æ¯”è¡¨æ ¼\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹\n",
    "    best_model = None\n",
    "    best_f1 = -1\n",
    "    \n",
    "    for model_name, results in experiment_results.items():\n",
    "        if 'f1' in results and not np.isnan(results['f1']):\n",
    "            if results['f1'] > best_f1:\n",
    "                best_f1 = results['f1']\n",
    "                best_model = model_name\n",
    "    \n",
    "    if best_model:\n",
    "        print(f\"\\næœ€ä½³æ¨¡å‹: {best_model} (F1 Score: {best_f1:.4f})\")\n",
    "else:\n",
    "    print(\"æ²¡æœ‰æˆåŠŸçš„å®éªŒç»“æœ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–ç»“æœ\n",
    "if experiment_results and config['output']['generate_plots']:\n",
    "    print(\"\\nGenerating visualization charts\")\n",
    "    \n",
    "    # Set plot style\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Time Series Anomaly Detection Benchmark Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Performance metrics bar chart\n",
    "    ax1 = axes[0, 0]\n",
    "    metrics_to_plot = ['f1', 'precision', 'recall']\n",
    "    model_names = list(experiment_results.keys())\n",
    "    \n",
    "    x = np.arange(len(metrics_to_plot))\n",
    "    width = 0.35 if len(model_names) == 2 else 0.25\n",
    "    \n",
    "    for i, model_name in enumerate(model_names):\n",
    "        values = []\n",
    "        for metric in metrics_to_plot:\n",
    "            value = experiment_results[model_name].get(metric, 0)\n",
    "            if isinstance(value, float) and not np.isnan(value):\n",
    "                values.append(value)\n",
    "            else:\n",
    "                values.append(0)\n",
    "        \n",
    "        ax1.bar(x + i * width, values, width, label=model_name, alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('Metric')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_title('Performance Comparison')\n",
    "    ax1.set_xticks(x + width * (len(model_names) - 1) / 2)\n",
    "    ax1.set_xticklabels(metrics_to_plot)\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Anomaly score distribution (example for first model)\n",
    "   \n",
    "    \n",
    "    # 3. Time series anomaly detection example (first 200 points)\n",
    "    ax3 = axes[1, 0]\n",
    "    n_points = min(200, len(test_labels))\n",
    "    time_indices = np.arange(n_points)\n",
    "    \n",
    "    # Show data for the first feature\n",
    "    ax3.plot(time_indices, test_data[:n_points, 0], 'b-', alpha=0.7, label='Time Series')\n",
    "    \n",
    "    # Mark anomaly points\n",
    "    anomaly_indices = np.where(test_labels[:n_points] == 1)[0]\n",
    "    if len(anomaly_indices) > 0:\n",
    "        ax3.scatter(anomaly_indices, test_data[anomaly_indices, 0], \n",
    "                   color='red', s=50, label='True Anomaly', zorder=5)\n",
    "    \n",
    "    ax3.set_xlabel('Time Index')\n",
    "    ax3.set_ylabel('Feature Value')\n",
    "    ax3.set_title('Time Series Anomaly Detection Example')\n",
    "    ax3.legend()\n",
    "    ax3.grid(alpha=0.3)\n",
    "    \n",
    "    # 4. Confusion matrix (for best model)\n",
    "    if best_model:\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        import itertools\n",
    "        \n",
    "        ax4 = axes[1, 1]\n",
    "        best_results = experiment_results[best_model]\n",
    "        \n",
    "        # Get best model predictions (binarized)\n",
    "        best_model_obj = model_objects[best_model]\n",
    "        if best_model == 'iforest':\n",
    "            best_pred = best_model_obj.predict(test_data)\n",
    "        elif best_model == 'lstm_ae':\n",
    "            best_pred = (best_model_obj.predict(test_data, metadata) > np.percentile(best_model_obj.predict(train_data, metadata), 90)).astype(int)\n",
    "        else:\n",
    "            best_pred = np.zeros_like(test_labels)\n",
    "        \n",
    "        cm = confusion_matrix(test_labels, best_pred)\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax4)\n",
    "        ax4.set_xlabel('Predicted Label')\n",
    "        ax4.set_ylabel('True Label')\n",
    "        ax4.set_title(f'{best_model} Confusion Matrix')\n",
    "        ax4.xaxis.set_ticklabels(['Normal', 'Anomaly'])\n",
    "        ax4.yaxis.set_ticklabels(['Normal', 'Anomaly'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Visualization charts generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc99929",
   "metadata": {},
   "source": [
    "## 5. ç»“æœä¿å­˜ä¸æŠ¥å‘Šç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜å®éªŒç»“æœ\n",
    "if config['output']['save_results'] and experiment_results:\n",
    "    print(\"\\nSaving experiment results\")\n",
    "    \n",
    "    # Create results directory\n",
    "    results_dir = project_root / config['output']['results_dir']\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Generate timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    experiment_name = config['experiment']['name']\n",
    "    \n",
    "    # ä¿å­˜è¯¦ç»†ç»“æœï¼Œä¿®æ­£å…ƒæ•°æ®å­—æ®µ\n",
    "    train_size = train_data.shape[0] if 'train_data' in locals() else None\n",
    "    test_size = test_data.shape[0] if 'test_data' in locals() else None\n",
    "    anomaly_rate = (np.sum(test_labels) / len(test_labels)) if 'test_labels' in locals() else None\n",
    "    detailed_results = {\n",
    "        'experiment_info': config['experiment'],\n",
    "        'data_info': {\n",
    "            'dataset': getattr(metadata, 'dataset_name', 'unknown'),\n",
    "            'n_features': getattr(metadata, 'feature_dim', None),\n",
    "            'train_size': train_size,\n",
    "            'test_size': test_size,\n",
    "            'anomaly_rate': anomaly_rate,\n",
    "            'fault_type': getattr(metadata, 'fault_type', None)\n",
    "        },\n",
    "        'model_results': experiment_results,\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "print(\"ç»“æœä¿å­˜åŠŸèƒ½å¾…ç»†åŒ–\")\n",
    "\n",
    "    # ä¿å­˜ä¸ºYAMLæ–‡ä»¶\n",
    "    # results_file = results_dir / f\"{experiment_name}_{timestamp}.yaml\"\n",
    "    # with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    #     yaml.dump(detailed_results, f, default_flow_style=False, allow_unicode=True)\n",
    "    \n",
    "    # print(f\"Results saved to: {results_file}\")\n",
    "    \n",
    "    # # ä¿å­˜å¯¹æ¯”è¡¨æ ¼ä¸ºCSV\n",
    "    # if 'comparison_df' in locals():\n",
    "    #     csv_file = results_dir / f\"{experiment_name}_{timestamp}_comparison.csv\"\n",
    "    #     comparison_df.to_csv(csv_file, index=False)\n",
    "    #     print(f\"Comparison table saved to: {csv_file}\")\n",
    "    \n",
    "    # # ä¿å­˜å¯è§†åŒ–å›¾è¡¨\n",
    "    # if config['output']['generate_plots']:\n",
    "    #     plot_file = results_dir / f\"{experiment_name}_{timestamp}_plots.png\"\n",
    "    #     if 'fig' in locals():\n",
    "    #         fig.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "    #         print(f\"Plot saved to: {plot_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb395a",
   "metadata": {},
   "source": [
    "## 6. å®éªŒæ€»ç»“ä¸å»ºè®®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb53ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆå®éªŒæ€»ç»“æŠ¥å‘Š\n",
    "print(\"\\nå®éªŒæ€»ç»“æŠ¥å‘Š\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"å®éªŒåç§°: {config['experiment']['name']}\")\n",
    "print(f\"æ•°æ®é›†: {metadata.dataset_name}\")\n",
    "\n",
    "print(f\"è¯„ä¼°æŒ‡æ ‡: {', '.join(config['evaluation']['metrics'])}\")\n",
    "\n",
    "if experiment_results:\n",
    "    print(f\"\\nä¸»è¦å‘ç°:\")\n",
    "    \n",
    "    # åˆ†æå„æ¨¡å‹æ€§èƒ½\n",
    "    for model_name, results in experiment_results.items():\n",
    "        f1_score = results.get('f1', 0)\n",
    "        precision = results.get('precision', 0)\n",
    "        recall = results.get('recall', 0)\n",
    "        \n",
    "        print(f\" - {model_name}: F1={f1_score:.4f}, Precision={precision:.4f}, Recall={recall:.4f}\")\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "print(\"\\nåŸºå‡†æµ‹è¯•å®éªŒå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1523ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰¹é‡å®éªŒç¤ºä¾‹ï¼ˆå¯é€‰è¿è¡Œï¼‰\n",
    "def run_batch_experiments(base_config, param_grid):\n",
    "    \"\"\"\n",
    "    è¿è¡Œæ‰¹é‡å®éªŒè¿›è¡Œå‚æ•°æ‰«æ\n",
    "    \n",
    "    Args:\n",
    "        base_config: åŸºç¡€é…ç½®\n",
    "        param_grid: å‚æ•°ç½‘æ ¼\n",
    "    \"\"\"\n",
    "    batch_results = []\n",
    "    \n",
    "    for i, params in enumerate(param_grid):\n",
    "        print(f\"\\n æ‰¹é‡å®éªŒ {i+1}/{len(param_grid)}\")\n",
    "        \n",
    "        # æ›´æ–°é…ç½®\n",
    "        current_config = base_config.copy()\n",
    "        for key, value in params.items():\n",
    "            # æ”¯æŒåµŒå¥—é”®å¦‚ 'models.iforest.n_estimators'\n",
    "            keys = key.split('.')\n",
    "            target = current_config\n",
    "            for k in keys[:-1]:\n",
    "                target = target[k]\n",
    "            target[keys[-1]] = value\n",
    "        \n",
    "        # è¿è¡Œå®éªŒï¼ˆè¿™é‡Œç®€åŒ–ä¸ºç¤ºä¾‹ï¼‰\n",
    "        print(f\"   å‚æ•°: {params}\")\n",
    "        \n",
    "        # TODO: åœ¨è¿™é‡Œæ·»åŠ å®Œæ•´çš„å®éªŒæµç¨‹\n",
    "        # è¿™éœ€è¦é‡æ„ä¸Šé¢çš„ä»£ç ä¸ºå‡½æ•°å½¢å¼\n",
    "        \n",
    "        batch_results.append({\n",
    "            'params': params,\n",
    "            'results': {}  # å®é™…ç»“æœ\n",
    "        })\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "# ç¤ºä¾‹å‚æ•°ç½‘æ ¼ï¼ˆå®é™…ä½¿ç”¨æ—¶å–æ¶ˆæ³¨é‡Šï¼‰\n",
    "# param_grid = [\n",
    "#     {'models.iforest.n_estimators': 50, 'models.iforest.contamination': 0.05},\n",
    "#     {'models.iforest.n_estimators': 100, 'models.iforest.contamination': 0.1},\n",
    "#     {'models.iforest.n_estimators': 200, 'models.iforest.contamination': 0.15},\n",
    "# ]\n",
    "\n",
    "# # è¿è¡Œæ‰¹é‡å®éªŒ\n",
    "# batch_results = run_batch_experiments(config, param_grid)\n",
    "\n",
    "print(\"æ‰¹é‡å®éªŒåŠŸèƒ½å¾…å®šä¹‰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_py311_rocm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
