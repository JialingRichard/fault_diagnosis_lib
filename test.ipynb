{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57c546e",
   "metadata": {},
   "source": [
    "# æ—¶åºæ•…éšœæ£€æµ‹æ¨¡å‹ Benchmark æ¶æ„åˆ†æ\n",
    "\n",
    "## 1. æ¶æ„ä¼˜åŠ¿åˆ†æ\n",
    "\n",
    "### ğŸ¯ æ ¸å¿ƒè®¾è®¡ç†å¿µçš„ä¼˜åŠ¿\n",
    "æ‚¨çš„\"ç”¨ç»Ÿä¸€çš„æ¥å£å¤„ç†å·®å¼‚ï¼Œç”¨å…ƒæ•°æ®æŒ‡å¯¼æµç¨‹\"ç†å¿µéå¸¸å‡ºè‰²ï¼Œä½“ç°åœ¨ï¼š\n",
    "\n",
    "- **æŠ½è±¡å±‚æ¬¡æ°å½“**: æ—¢ä¿æŒäº†çµæ´»æ€§ï¼Œåˆå®ç°äº†æ ‡å‡†åŒ–\n",
    "- **å…ƒæ•°æ®é©±åŠ¨**: é€šè¿‡æ•°æ®ç‰¹å¾è‡ªåŠ¨è°ƒæ•´å¤„ç†æµç¨‹ï¼Œé¿å…ç¡¬ç¼–ç \n",
    "- **æ¨¡å—åŒ–è®¾è®¡**: å„ç»„ä»¶èŒè´£æ¸…æ™°ï¼Œæ˜“äºç»´æŠ¤å’Œæ‰©å±•\n",
    "\n",
    "### ğŸ—ï¸ äº”å¤§æ ¸å¿ƒç»„ä»¶åˆ†æ\n",
    "\n",
    "#### 1. é…ç½®ç®¡ç†å™¨ (Config Manager)\n",
    "**ä¼˜åŠ¿**:\n",
    "- æ”¯æŒå®éªŒå¯å¤ç°æ€§\n",
    "- ä¾¿äºæ‰¹é‡å®éªŒå’Œå‚æ•°è°ƒä¼˜\n",
    "- é™ä½ä½¿ç”¨é—¨æ§›\n",
    "\n",
    "**å»ºè®®å¢å¼º**:\n",
    "```yaml\n",
    "# ç¤ºä¾‹é…ç½®ç»“æ„\n",
    "experiment:\n",
    "  name: \"lstm_vs_isolation_forest\"\n",
    "  output_dir: \"./results\"\n",
    "  \n",
    "dataset:\n",
    "  name: \"swat_dataset\"\n",
    "  path: \"./data/swat\"\n",
    "  preprocessing:\n",
    "    normalization: \"minmax\"\n",
    "    window_size: 100\n",
    "    \n",
    "model:\n",
    "  type: \"LSTM\"\n",
    "  hyperparameters:\n",
    "    hidden_size: 64\n",
    "    num_layers: 2\n",
    "    dropout: 0.1\n",
    "    \n",
    "evaluation:\n",
    "  metrics: [\"f1_point_adjusted\", \"precision\", \"recall\"]\n",
    "  threshold_strategy: \"best_f1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e483ae6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "è®¡ç®—å®Œæˆï¼Œç»“æœè®¾å¤‡: cuda:0\n",
      "GPU æ˜¾å­˜ä½¿ç”¨: 0.04 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6209/1093244567.py:10: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:310.)\n",
      "  z = torch.matmul(x, y)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# æ£€æŸ¥è®¾å¤‡\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# æµ‹è¯•ç®€å•æ“ä½œ\n",
    "x = torch.randn(1000, 1000).to(device)\n",
    "y = torch.randn(1000, 1000).to(device)\n",
    "z = torch.matmul(x, y)\n",
    "print(f\"è®¡ç®—å®Œæˆï¼Œç»“æœè®¾å¤‡: {z.device}\")\n",
    "print(f\"GPU æ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6a2c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. æ•°æ®ç®¡é“ (Data Pipeline) æ¶æ„ç¤ºä¾‹\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Union, Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "@dataclass\n",
    "class DataMetadata:\n",
    "    \"\"\"æ•°æ®å…ƒæ•°æ®ç±»ï¼Œç”¨äºæè¿°æ•°æ®ç‰¹å¾\"\"\"\n",
    "    label_granularity: str  # \"point-wise\" or \"sequence-wise\"\n",
    "    fault_type: str         # \"binary\" or \"multi-class\"\n",
    "    num_classes: int        # ç±»åˆ«æ•°é‡\n",
    "    sequence_length: int    # åºåˆ—é•¿åº¦\n",
    "    feature_dim: int        # ç‰¹å¾ç»´åº¦\n",
    "    dataset_name: str       # æ•°æ®é›†åç§°\n",
    "    \n",
    "class BaseDataLoader(ABC):\n",
    "    \"\"\"ç»Ÿä¸€çš„æ•°æ®åŠ è½½å™¨æŠ½è±¡åŸºç±»\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_data(self) -> tuple:\n",
    "        \"\"\"åŠ è½½æ•°æ®ï¼Œè¿”å› (X, y, metadata)\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_metadata(self) -> DataMetadata:\n",
    "        \"\"\"è·å–æ•°æ®å…ƒæ•°æ®\"\"\"\n",
    "        pass\n",
    "\n",
    "class DataPipeline:\n",
    "    \"\"\"ç»Ÿä¸€æ•°æ®ç®¡é“\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.metadata = None\n",
    "        \n",
    "    def prepare_data(self) -> tuple:\n",
    "        \"\"\"\n",
    "        å‡†å¤‡æ•°æ®çš„ä¸»è¦å…¥å£\n",
    "        è¿”å›: (X_train, X_test, y_train, y_test, metadata)\n",
    "        \"\"\"\n",
    "        # 1. æ ¹æ®é…ç½®é€‰æ‹©åˆé€‚çš„æ•°æ®åŠ è½½å™¨\n",
    "        loader = self._get_data_loader()\n",
    "        \n",
    "        # 2. åŠ è½½åŸå§‹æ•°æ®\n",
    "        X, y, self.metadata = loader.load_data()\n",
    "        \n",
    "        # 3. æ•°æ®é¢„å¤„ç†\n",
    "        X_processed = self._preprocess(X)\n",
    "        \n",
    "        # 4. æ•°æ®åˆ’åˆ†\n",
    "        X_train, X_test, y_train, y_test = self._split_data(X_processed, y)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, self.metadata\n",
    "    \n",
    "    def _get_data_loader(self):\n",
    "        \"\"\"æ ¹æ®é…ç½®åˆ›å»ºæ•°æ®åŠ è½½å™¨\"\"\"\n",
    "        dataset_name = self.config['dataset']['name']\n",
    "        \n",
    "        # å·¥å‚æ¨¡å¼åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "        loader_map = {\n",
    "            'swat': SwatDataLoader,\n",
    "            'smd': SMDDataLoader,\n",
    "            'msl': MSLDataLoader,\n",
    "            # æ›´å¤šæ•°æ®é›†...\n",
    "        }\n",
    "        \n",
    "        if dataset_name not in loader_map:\n",
    "            raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "            \n",
    "        return loader_map[dataset_name](self.config['dataset'])\n",
    "\n",
    "# ç¤ºä¾‹ï¼šå…·ä½“æ•°æ®é›†åŠ è½½å™¨\n",
    "class SwatDataLoader(BaseDataLoader):\n",
    "    \"\"\"SWATæ•°æ®é›†åŠ è½½å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "    def load_data(self) -> tuple:\n",
    "        # åŠ è½½SWATæ•°æ®é›†çš„å…·ä½“å®ç°\n",
    "        data_path = self.config['path']\n",
    "        \n",
    "        # è¿™é‡Œæ˜¯ç¤ºä¾‹ï¼Œå®é™…éœ€è¦æ ¹æ®æ•°æ®æ ¼å¼å®ç°\n",
    "        X = np.random.randn(1000, 51)  # ç¤ºä¾‹æ•°æ®\n",
    "        y = np.random.randint(0, 2, 1000)  # ç¤ºä¾‹æ ‡ç­¾\n",
    "        \n",
    "        metadata = DataMetadata(\n",
    "            label_granularity=\"point-wise\",\n",
    "            fault_type=\"binary\", \n",
    "            num_classes=2,\n",
    "            sequence_length=1000,\n",
    "            feature_dim=51,\n",
    "            dataset_name=\"swat\"\n",
    "        )\n",
    "        \n",
    "        return X, y, metadata\n",
    "    \n",
    "    def get_metadata(self) -> DataMetadata:\n",
    "        return self.metadata\n",
    "\n",
    "print(\"âœ… æ•°æ®ç®¡é“æ¶æ„è®¾è®¡å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88344e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. æ¨¡å‹ä¸­å¿ƒ (Model Hub) æ¶æ„è®¾è®¡\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, Dict, Any\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    \"\"\"ç»Ÿä¸€æ¨¡å‹æŠ½è±¡åŸºç±»\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.is_trained = False\n",
    "        \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def requires_training_loop(self) -> bool:\n",
    "        \"\"\"æŒ‡ç¤ºæ˜¯å¦éœ€è¦å¤æ‚çš„è®­ç»ƒå¾ªç¯ï¼ˆæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸ºTrueï¼‰\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def build_model(self, input_shape: tuple) -> None:\n",
    "        \"\"\"æ„å»ºæ¨¡å‹\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self, X_train, y_train=None) -> None:\n",
    "        \"\"\"ç®€å•æ¨¡å‹çš„è®­ç»ƒæ–¹æ³•\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict_anomaly_score(self, X) -> np.ndarray:\n",
    "        \"\"\"é¢„æµ‹å¼‚å¸¸åˆ†æ•°\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_model_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"è·å–æ¨¡å‹ä¿¡æ¯\"\"\"\n",
    "        return {\n",
    "            'name': self.__class__.__name__,\n",
    "            'requires_training_loop': self.requires_training_loop,\n",
    "            'is_trained': self.is_trained,\n",
    "            'config': self.config\n",
    "        }\n",
    "\n",
    "# ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ç¤ºä¾‹\n",
    "class IsolationForestModel(BaseModel):\n",
    "    \"\"\"Isolation Forestæ¨¡å‹åŒ…è£…å™¨\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def requires_training_loop(self) -> bool:\n",
    "        return False  # ä¸éœ€è¦å¤æ‚è®­ç»ƒå¾ªç¯\n",
    "    \n",
    "    def build_model(self, input_shape: tuple) -> None:\n",
    "        self.model = IsolationForest(\n",
    "            contamination=self.config.get('contamination', 0.1),\n",
    "            random_state=self.config.get('random_state', 42),\n",
    "            n_estimators=self.config.get('n_estimators', 100)\n",
    "        )\n",
    "    \n",
    "    def fit(self, X_train, y_train=None) -> None:\n",
    "        \"\"\"Isolation Foreståªéœ€è¦æ­£å¸¸æ•°æ®è®­ç»ƒ\"\"\"\n",
    "        if not self.model:\n",
    "            self.build_model(X_train.shape)\n",
    "        \n",
    "        # å¯¹äºæ— ç›‘ç£æ¨¡å‹ï¼Œé€šå¸¸åªç”¨æ­£å¸¸æ•°æ®è®­ç»ƒ\n",
    "        normal_data = X_train[y_train == 0] if y_train is not None else X_train\n",
    "        self.model.fit(normal_data)\n",
    "        self.is_trained = True\n",
    "    \n",
    "    def predict_anomaly_score(self, X) -> np.ndarray:\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained first\")\n",
    "        \n",
    "        # Isolation Forestè¿”å›çš„æ˜¯è´Ÿçš„å¼‚å¸¸åˆ†æ•°ï¼Œéœ€è¦è½¬æ¢\n",
    "        scores = -self.model.decision_function(X)\n",
    "        return scores\n",
    "\n",
    "# æ·±åº¦å­¦ä¹ æ¨¡å‹ç¤ºä¾‹\n",
    "class LSTMModel(BaseModel):\n",
    "    \"\"\"LSTMå¼‚å¸¸æ£€æµ‹æ¨¡å‹\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def requires_training_loop(self) -> bool:\n",
    "        return True  # éœ€è¦å¤æ‚è®­ç»ƒå¾ªç¯\n",
    "    \n",
    "    def build_model(self, input_shape: tuple) -> None:\n",
    "        \"\"\"æ„å»ºLSTMæ¨¡å‹\"\"\"\n",
    "        seq_len, feature_dim = input_shape[1], input_shape[2]\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.LSTM(\n",
    "                input_size=feature_dim,\n",
    "                hidden_size=self.config.get('hidden_size', 64),\n",
    "                num_layers=self.config.get('num_layers', 2),\n",
    "                batch_first=True,\n",
    "                dropout=self.config.get('dropout', 0.1)\n",
    "            )[0],  # åªè¦LSTMå±‚ï¼Œä¸è¦hidden state\n",
    "            nn.Linear(self.config.get('hidden_size', 64), feature_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # å®šä¹‰æŸå¤±å‡½æ•°\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=self.config.get('learning_rate', 0.001)\n",
    "        )\n",
    "    \n",
    "    def fit(self, X_train, y_train=None) -> None:\n",
    "        \"\"\"ç®€å•çš„fitæ–¹æ³•ï¼Œå¤æ‚è®­ç»ƒå°†ç”±Trainerå¤„ç†\"\"\"\n",
    "        if not self.model:\n",
    "            self.build_model(X_train.shape)\n",
    "        \n",
    "        # å¯¹äºéœ€è¦è®­ç»ƒå¾ªç¯çš„æ¨¡å‹ï¼Œè¿™é‡Œåªæ˜¯å ä½\n",
    "        # å®é™…è®­ç»ƒå°†ç”±Trainerç±»å®Œæˆ\n",
    "        print(\"LSTM model initialized, training will be handled by Trainer\")\n",
    "    \n",
    "    def predict_anomaly_score(self, X) -> np.ndarray:\n",
    "        \"\"\"è®¡ç®—é‡æ„è¯¯å·®ä½œä¸ºå¼‚å¸¸åˆ†æ•°\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained first\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X)\n",
    "            reconstructed = self.model(X_tensor)\n",
    "            \n",
    "            # è®¡ç®—é‡æ„è¯¯å·®\n",
    "            mse = nn.MSELoss(reduction='none')\n",
    "            errors = mse(reconstructed, X_tensor)\n",
    "            anomaly_scores = torch.mean(errors, dim=(1, 2)).numpy()\n",
    "            \n",
    "        return anomaly_scores\n",
    "\n",
    "# æ¨¡å‹å·¥å‚\n",
    "class ModelFactory:\n",
    "    \"\"\"æ¨¡å‹å·¥å‚ç±»\"\"\"\n",
    "    \n",
    "    _models = {\n",
    "        'isolation_forest': IsolationForestModel,\n",
    "        'lstm': LSTMModel,\n",
    "        # å¯ä»¥ç»§ç»­æ·»åŠ æ›´å¤šæ¨¡å‹...\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def create_model(cls, model_type: str, config: Dict[str, Any]) -> BaseModel:\n",
    "        \"\"\"åˆ›å»ºæŒ‡å®šç±»å‹çš„æ¨¡å‹\"\"\"\n",
    "        if model_type not in cls._models:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        \n",
    "        return cls._models[model_type](config)\n",
    "    \n",
    "    @classmethod\n",
    "    def list_available_models(cls) -> list:\n",
    "        \"\"\"åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹ç±»å‹\"\"\"\n",
    "        return list(cls._models.keys())\n",
    "\n",
    "print(\"âœ… æ¨¡å‹ä¸­å¿ƒæ¶æ„è®¾è®¡å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ab2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. è®­ç»ƒå™¨ (Trainer) å’Œè¯„ä¼°å™¨ (Evaluator) æ¶æ„\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def train_model(self, model: BaseModel, X_train: np.ndarray, \n",
    "                   y_train: np.ndarray = None) -> BaseModel:\n",
    "        \"\"\"è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹\"\"\"\n",
    "        \n",
    "        if not model.requires_training_loop:\n",
    "            raise ValueError(\"This model doesn't require training loop\")\n",
    "        \n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        train_loader = self._prepare_dataloader(X_train, y_train)\n",
    "        \n",
    "        # è®­ç»ƒé…ç½®\n",
    "        epochs = self.config.get('epochs', 100)\n",
    "        patience = self.config.get('patience', 10)\n",
    "        \n",
    "        model.model.to(self.device)\n",
    "        model.model.train()\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        print(f\"å¼€å§‹è®­ç»ƒï¼Œè®¾å¤‡: {self.device}\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            \n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X = batch_X.to(self.device)\n",
    "                \n",
    "                model.optimizer.zero_grad()\n",
    "                \n",
    "                # å‰å‘ä¼ æ’­\n",
    "                if hasattr(model.model, 'lstm'):\n",
    "                    # LSTMæ¨¡å‹çš„ç‰¹æ®Šå¤„ç†\n",
    "                    reconstructed = model.model(batch_X)\n",
    "                    loss = model.criterion(reconstructed, batch_X)\n",
    "                else:\n",
    "                    # å…¶ä»–æ¨¡å‹\n",
    "                    output = model.model(batch_X)\n",
    "                    loss = model.criterion(output, batch_X)\n",
    "                \n",
    "                # åå‘ä¼ æ’­\n",
    "                loss.backward()\n",
    "                model.optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            avg_loss = epoch_loss / len(train_loader)\n",
    "            \n",
    "            # æ—©åœç­–ç•¥\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                patience_counter = 0\n",
    "                # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "                self._save_best_model(model)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"æ—©åœåœ¨ç¬¬ {epoch+1} è½®ï¼Œæœ€ä½³æŸå¤±: {best_loss:.6f}\")\n",
    "                break\n",
    "                \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "        \n",
    "        # åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "        self._load_best_model(model)\n",
    "        model.is_trained = True\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _prepare_dataloader(self, X: np.ndarray, y: np.ndarray = None) -> DataLoader:\n",
    "        \"\"\"å‡†å¤‡æ•°æ®åŠ è½½å™¨\"\"\"\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        \n",
    "        if y is not None:\n",
    "            y_tensor = torch.FloatTensor(y)\n",
    "            dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        else:\n",
    "            dataset = TensorDataset(X_tensor, X_tensor)  # è‡ªç¼–ç å™¨è®­ç»ƒ\n",
    "            \n",
    "        return DataLoader(\n",
    "            dataset, \n",
    "            batch_size=self.config.get('batch_size', 32),\n",
    "            shuffle=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "    \n",
    "    def _save_best_model(self, model: BaseModel):\n",
    "        \"\"\"ä¿å­˜æœ€ä½³æ¨¡å‹\"\"\"\n",
    "        # å®é™…åº”ç”¨ä¸­åº”è¯¥ä¿å­˜åˆ°æ–‡ä»¶\n",
    "        self.best_model_state = model.model.state_dict().copy()\n",
    "    \n",
    "    def _load_best_model(self, model: BaseModel):\n",
    "        \"\"\"åŠ è½½æœ€ä½³æ¨¡å‹\"\"\"\n",
    "        if hasattr(self, 'best_model_state'):\n",
    "            model.model.load_state_dict(self.best_model_state)\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"æ™ºèƒ½è¯„ä¼°å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.supported_metrics = [\n",
    "            'precision', 'recall', 'f1', 'f1_point_adjusted', \n",
    "            'auc', 'accuracy', 'best_f1_threshold'\n",
    "        ]\n",
    "    \n",
    "    def evaluate(self, y_true: np.ndarray, anomaly_scores: np.ndarray, \n",
    "                metadata: DataMetadata) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        æ ¹æ®å…ƒæ•°æ®è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„è¯„ä¼°æ–¹æ³•\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # æ ¹æ®æ ‡ç­¾ç²’åº¦å’Œæ•…éšœç±»å‹é€‰æ‹©è¯„ä¼°ç­–ç•¥\n",
    "        if metadata.label_granularity == \"point-wise\":\n",
    "            if metadata.fault_type == \"binary\":\n",
    "                results = self._evaluate_binary_pointwise(y_true, anomaly_scores)\n",
    "            else:\n",
    "                results = self._evaluate_multiclass_pointwise(y_true, anomaly_scores)\n",
    "        else:  # sequence-wise\n",
    "            if metadata.fault_type == \"binary\":\n",
    "                results = self._evaluate_binary_sequence(y_true, anomaly_scores)\n",
    "            else:\n",
    "                results = self._evaluate_multiclass_sequence(y_true, anomaly_scores)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _evaluate_binary_pointwise(self, y_true: np.ndarray, \n",
    "                                  anomaly_scores: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"äºŒåˆ†ç±»é€ç‚¹è¯„ä¼°\"\"\"\n",
    "        \n",
    "        # æ‰¾åˆ°æœ€ä½³é˜ˆå€¼\n",
    "        best_threshold, best_f1 = self._find_best_threshold(y_true, anomaly_scores)\n",
    "        y_pred = (anomaly_scores > best_threshold).astype(int)\n",
    "        \n",
    "        results = {\n",
    "            'best_threshold': best_threshold,\n",
    "            'precision': precision_score(y_true, y_pred),\n",
    "            'recall': recall_score(y_true, y_pred),\n",
    "            'f1': f1_score(y_true, y_pred),\n",
    "            'f1_point_adjusted': self._point_adjusted_f1(y_true, y_pred),\n",
    "            'auc': roc_auc_score(y_true, anomaly_scores)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _point_adjusted_f1(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"ç‚¹è°ƒæ•´F1åˆ†æ•° - ç”¨äºæ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹\"\"\"\n",
    "        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…å®ç°ä¼šæ›´å¤æ‚\n",
    "        # Point-adjustedè¯„ä¼°ä¼šè€ƒè™‘å¼‚å¸¸åŒºé—´çš„è¿ç»­æ€§\n",
    "        \n",
    "        # æ‰¾åˆ°çœŸå®å¼‚å¸¸åŒºé—´\n",
    "        true_anomaly_ranges = self._find_anomaly_ranges(y_true)\n",
    "        pred_anomaly_ranges = self._find_anomaly_ranges(y_pred)\n",
    "        \n",
    "        # è®¡ç®—åŒºé—´çº§åˆ«çš„TP, FP, FN\n",
    "        tp = 0\n",
    "        for true_range in true_anomaly_ranges:\n",
    "            for pred_range in pred_anomaly_ranges:\n",
    "                if self._ranges_overlap(true_range, pred_range):\n",
    "                    tp += 1\n",
    "                    break\n",
    "        \n",
    "        fp = len(pred_anomaly_ranges) - tp\n",
    "        fn = len(true_anomaly_ranges) - tp\n",
    "        \n",
    "        if tp + fp == 0:\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "            \n",
    "        if tp + fn == 0:\n",
    "            recall = 0\n",
    "        else:\n",
    "            recall = tp / (tp + fn)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    def _find_best_threshold(self, y_true: np.ndarray, \n",
    "                           anomaly_scores: np.ndarray) -> Tuple[float, float]:\n",
    "        \"\"\"æ‰¾åˆ°æœ€ä½³F1åˆ†æ•°å¯¹åº”çš„é˜ˆå€¼\"\"\"\n",
    "        thresholds = np.linspace(anomaly_scores.min(), anomaly_scores.max(), 100)\n",
    "        best_f1 = 0\n",
    "        best_threshold = thresholds[0]\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            y_pred = (anomaly_scores > threshold).astype(int)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        return best_threshold, best_f1\n",
    "    \n",
    "    def _find_anomaly_ranges(self, y: np.ndarray) -> List[Tuple[int, int]]:\n",
    "        \"\"\"æ‰¾åˆ°å¼‚å¸¸åŒºé—´\"\"\"\n",
    "        ranges = []\n",
    "        start = None\n",
    "        \n",
    "        for i, val in enumerate(y):\n",
    "            if val == 1 and start is None:  # å¼‚å¸¸å¼€å§‹\n",
    "                start = i\n",
    "            elif val == 0 and start is not None:  # å¼‚å¸¸ç»“æŸ\n",
    "                ranges.append((start, i-1))\n",
    "                start = None\n",
    "        \n",
    "        # å¤„ç†åºåˆ—æœ«å°¾çš„å¼‚å¸¸\n",
    "        if start is not None:\n",
    "            ranges.append((start, len(y)-1))\n",
    "        \n",
    "        return ranges\n",
    "    \n",
    "    def _ranges_overlap(self, range1: Tuple[int, int], range2: Tuple[int, int]) -> bool:\n",
    "        \"\"\"åˆ¤æ–­ä¸¤ä¸ªåŒºé—´æ˜¯å¦é‡å \"\"\"\n",
    "        return max(range1[0], range2[0]) <= min(range1[1], range2[1])\n",
    "    \n",
    "    def _evaluate_multiclass_pointwise(self, y_true, anomaly_scores):\n",
    "        \"\"\"å¤šç±»åˆ«é€ç‚¹è¯„ä¼° - å¾…å®ç°\"\"\"\n",
    "        # å¤šç±»åˆ«è¯„ä¼°é€»è¾‘\n",
    "        pass\n",
    "    \n",
    "    def _evaluate_binary_sequence(self, y_true, anomaly_scores):\n",
    "        \"\"\"äºŒåˆ†ç±»åºåˆ—çº§è¯„ä¼° - å¾…å®ç°\"\"\"\n",
    "        # åºåˆ—çº§è¯„ä¼°é€»è¾‘\n",
    "        pass\n",
    "    \n",
    "    def _evaluate_multiclass_sequence(self, y_true, anomaly_scores):\n",
    "        \"\"\"å¤šç±»åˆ«åºåˆ—çº§è¯„ä¼° - å¾…å®ç°\"\"\"\n",
    "        # å¤šç±»åˆ«åºåˆ—çº§è¯„ä¼°é€»è¾‘\n",
    "        pass\n",
    "\n",
    "print(\"âœ… è®­ç»ƒå™¨å’Œè¯„ä¼°å™¨æ¶æ„è®¾è®¡å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a37633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ä¸»ç¨‹åº (Main) - å®Œæ•´å·¥ä½œæµç¨‹ç¤ºä¾‹\n",
    "\n",
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class FaultDiagnosisBenchmark:\n",
    "    \"\"\"æ•…éšœè¯Šæ–­æ¨¡å‹ Benchmark ä¸»ç±»\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str):\n",
    "        \"\"\"åˆå§‹åŒ– Benchmark\"\"\"\n",
    "        with open(config_path, 'r') as f:\n",
    "            self.config = yaml.safe_load(f)\n",
    "        \n",
    "        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶\n",
    "        self.data_pipeline = DataPipeline(self.config)\n",
    "        self.trainer = Trainer(self.config.get('training', {}))\n",
    "        self.evaluator = Evaluator()\n",
    "        \n",
    "        # ç»“æœå­˜å‚¨\n",
    "        self.results = {}\n",
    "    \n",
    "    def run_experiment(self) -> Dict[str, Any]:\n",
    "        \"\"\"è¿è¡Œå®Œæ•´çš„å®éªŒæµç¨‹\"\"\"\n",
    "        \n",
    "        print(\"ğŸš€ å¼€å§‹è¿è¡Œæ•…éšœè¯Šæ–­æ¨¡å‹ Benchmark\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. æ•°æ®å‡†å¤‡\n",
    "        print(\"ğŸ“Š å‡†å¤‡æ•°æ®...\")\n",
    "        X_train, X_test, y_train, y_test, metadata = self.data_pipeline.prepare_data()\n",
    "        \n",
    "        print(f\"âœ… æ•°æ®åŠ è½½å®Œæˆ:\")\n",
    "        print(f\"   - æ•°æ®é›†: {metadata.dataset_name}\")\n",
    "        print(f\"   - æ ‡ç­¾ç²’åº¦: {metadata.label_granularity}\")\n",
    "        print(f\"   - æ•…éšœç±»å‹: {metadata.fault_type}\")\n",
    "        print(f\"   - è®­ç»ƒé›†å¤§å°: {X_train.shape}\")\n",
    "        print(f\"   - æµ‹è¯•é›†å¤§å°: {X_test.shape}\")\n",
    "        print()\n",
    "        \n",
    "        # 2. æ¨¡å‹åˆ›å»º\n",
    "        print(\"ğŸ¤– åˆ›å»ºæ¨¡å‹...\")\n",
    "        model_config = self.config['model']\n",
    "        model = ModelFactory.create_model(\n",
    "            model_config['type'].lower(), \n",
    "            model_config.get('hyperparameters', {})\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ: {model.__class__.__name__}\")\n",
    "        print(f\"   - éœ€è¦è®­ç»ƒå¾ªç¯: {model.requires_training_loop}\")\n",
    "        print()\n",
    "        \n",
    "        # 3. æ¨¡å‹è®­ç»ƒ\n",
    "        print(\"ğŸ¯ å¼€å§‹è®­ç»ƒ...\")\n",
    "        if model.requires_training_loop:\n",
    "            # æ·±åº¦å­¦ä¹ æ¨¡å‹ä½¿ç”¨è®­ç»ƒå™¨\n",
    "            print(\"   ä½¿ç”¨è®­ç»ƒå™¨è¿›è¡Œæ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒ...\")\n",
    "            model = self.trainer.train_model(model, X_train, y_train)\n",
    "        else:\n",
    "            # ä¼ ç»Ÿæ¨¡å‹ç›´æ¥è°ƒç”¨fit\n",
    "            print(\"   ä½¿ç”¨ç®€å•fitæ–¹æ³•è®­ç»ƒä¼ ç»Ÿæ¨¡å‹...\")\n",
    "            model.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ\")\n",
    "        print()\n",
    "        \n",
    "        # 4. æ¨¡å‹è¯„ä¼°\n",
    "        print(\"ğŸ“ˆ å¼€å§‹è¯„ä¼°...\")\n",
    "        anomaly_scores = model.predict_anomaly_score(X_test)\n",
    "        evaluation_results = self.evaluator.evaluate(y_test, anomaly_scores, metadata)\n",
    "        \n",
    "        print(\"âœ… è¯„ä¼°å®Œæˆ\")\n",
    "        print()\n",
    "        \n",
    "        # 5. ç»“æœæ•´ç†\n",
    "        self.results = {\n",
    "            'experiment_config': self.config,\n",
    "            'data_info': {\n",
    "                'dataset': metadata.dataset_name,\n",
    "                'label_granularity': metadata.label_granularity,\n",
    "                'fault_type': metadata.fault_type,\n",
    "                'train_size': X_train.shape[0],\n",
    "                'test_size': X_test.shape[0],\n",
    "                'feature_dim': metadata.feature_dim\n",
    "            },\n",
    "            'model_info': model.get_model_info(),\n",
    "            'evaluation_results': evaluation_results,\n",
    "            'anomaly_scores_stats': {\n",
    "                'mean': float(np.mean(anomaly_scores)),\n",
    "                'std': float(np.std(anomaly_scores)),\n",
    "                'min': float(np.min(anomaly_scores)),\n",
    "                'max': float(np.max(anomaly_scores))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 6. ä¿å­˜ç»“æœ\n",
    "        self._save_results()\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def _save_results(self):\n",
    "        \"\"\"ä¿å­˜å®éªŒç»“æœ\"\"\"\n",
    "        output_dir = Path(self.config['experiment'].get('output_dir', './results'))\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        experiment_name = self.config['experiment']['name']\n",
    "        result_file = output_dir / f\"{experiment_name}_results.json\"\n",
    "        \n",
    "        with open(result_file, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        \n",
    "        print(f\"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {result_file}\")\n",
    "    \n",
    "    def print_results(self):\n",
    "        \"\"\"æ‰“å°æ ¼å¼åŒ–çš„ç»“æœ\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"âŒ æ²¡æœ‰å¯ç”¨çš„ç»“æœ\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ“Š å®éªŒç»“æœæ±‡æ€»\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # æ•°æ®ä¿¡æ¯\n",
    "        data_info = self.results['data_info']\n",
    "        print(f\"ğŸ“ æ•°æ®é›†ä¿¡æ¯:\")\n",
    "        print(f\"   - åç§°: {data_info['dataset']}\")\n",
    "        print(f\"   - æ ‡ç­¾ç²’åº¦: {data_info['label_granularity']}\")\n",
    "        print(f\"   - æ•…éšœç±»å‹: {data_info['fault_type']}\")\n",
    "        print(f\"   - è®­ç»ƒæ ·æœ¬: {data_info['train_size']}\")\n",
    "        print(f\"   - æµ‹è¯•æ ·æœ¬: {data_info['test_size']}\")\n",
    "        print()\n",
    "        \n",
    "        # æ¨¡å‹ä¿¡æ¯\n",
    "        model_info = self.results['model_info']\n",
    "        print(f\"ğŸ¤– æ¨¡å‹ä¿¡æ¯:\")\n",
    "        print(f\"   - åç§°: {model_info['name']}\")\n",
    "        print(f\"   - è®­ç»ƒçŠ¶æ€: {'å·²è®­ç»ƒ' if model_info['is_trained'] else 'æœªè®­ç»ƒ'}\")\n",
    "        print()\n",
    "        \n",
    "        # è¯„ä¼°ç»“æœ\n",
    "        eval_results = self.results['evaluation_results']\n",
    "        print(f\"ğŸ“ˆ è¯„ä¼°ç»“æœ:\")\n",
    "        for metric, value in eval_results.items():\n",
    "            print(f\"   - {metric}: {value:.4f}\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "\n",
    "# ç¤ºä¾‹é…ç½®æ–‡ä»¶å†…å®¹\n",
    "example_config = {\n",
    "    'experiment': {\n",
    "        'name': 'lstm_swat_experiment',\n",
    "        'output_dir': './results'\n",
    "    },\n",
    "    'dataset': {\n",
    "        'name': 'swat',\n",
    "        'path': './data/swat',\n",
    "        'preprocessing': {\n",
    "            'normalization': 'minmax',\n",
    "            'window_size': 100\n",
    "        }\n",
    "    },\n",
    "    'model': {\n",
    "        'type': 'LSTM',\n",
    "        'hyperparameters': {\n",
    "            'hidden_size': 64,\n",
    "            'num_layers': 2,\n",
    "            'dropout': 0.1,\n",
    "            'learning_rate': 0.001\n",
    "        }\n",
    "    },\n",
    "    'training': {\n",
    "        'epochs': 50,\n",
    "        'batch_size': 32,\n",
    "        'patience': 10\n",
    "    },\n",
    "    'evaluation': {\n",
    "        'metrics': ['f1_point_adjusted', 'precision', 'recall', 'auc']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ… ä¸»ç¨‹åºæ¶æ„è®¾è®¡å®Œæˆ\")\n",
    "print(\"\\nğŸ‰ å®Œæ•´çš„æ•…éšœè¯Šæ–­ Benchmark æ¶æ„è®¾è®¡å®Œæ¯•ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71365a42",
   "metadata": {},
   "source": [
    "## 6. æ¶æ„ä¼˜åŠ¿ä¸æ½œåœ¨æŒ‘æˆ˜åˆ†æ\n",
    "\n",
    "### ğŸŒŸ ä¸»è¦ä¼˜åŠ¿\n",
    "\n",
    "#### 1. **ç»Ÿä¸€æ€§ä¸æ ‡å‡†åŒ–**\n",
    "- âœ… é€šè¿‡ç»Ÿä¸€çš„æ¥å£å¤„ç†ä¸åŒç±»å‹çš„æ¨¡å‹\n",
    "- âœ… æ ‡å‡†åŒ–çš„è¯„ä¼°æµç¨‹ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒ\n",
    "- âœ… é…ç½®é©±åŠ¨çš„å®éªŒç®¡ç†ï¼Œæé«˜å¯å¤ç°æ€§\n",
    "\n",
    "#### 2. **æ‰©å±•æ€§è®¾è®¡**\n",
    "- âœ… æ¨¡å—åŒ–æ¶æ„ä¾¿äºæ·»åŠ æ–°æ¨¡å‹ã€æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡\n",
    "- âœ… å·¥å‚æ¨¡å¼æ”¯æŒåŠ¨æ€æ¨¡å‹åˆ›å»º\n",
    "- âœ… æŠ½è±¡åŸºç±»ç¡®ä¿æ¥å£ä¸€è‡´æ€§\n",
    "\n",
    "#### 3. **æ™ºèƒ½åŒ–å¤„ç†**\n",
    "- âœ… å…ƒæ•°æ®é©±åŠ¨çš„è‡ªåŠ¨æµç¨‹é€‰æ‹©\n",
    "- âœ… æ ¹æ®æ•°æ®ç‰¹å¾è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„è¯„ä¼°æ–¹æ³•\n",
    "- âœ… æ™ºèƒ½é˜ˆå€¼é€‰æ‹©å’Œè¯„ä¼°æŒ‡æ ‡è®¡ç®—\n",
    "\n",
    "#### 4. **å®ç”¨æ€§ç‰¹è‰²**\n",
    "- âœ… æ”¯æŒä¼ ç»ŸMLå’Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç»Ÿä¸€ç®¡ç†\n",
    "- âœ… ä¸“é—¨çš„è®­ç»ƒå™¨å¤„ç†å¤æ‚è®­ç»ƒå¾ªç¯\n",
    "- âœ… Point-Adjustedè¯„ä¼°é€‚é…æ—¶åºå¼‚å¸¸æ£€æµ‹ç‰¹ç‚¹\n",
    "\n",
    "### âš ï¸ æ½œåœ¨æŒ‘æˆ˜ä¸æ”¹è¿›å»ºè®®\n",
    "\n",
    "#### 1. **æ€§èƒ½ä¼˜åŒ–**\n",
    "**æŒ‘æˆ˜**: å¤§è§„æ¨¡æ•°æ®é›†çš„å†…å­˜å’Œè®¡ç®—æ•ˆç‡\n",
    "**å»ºè®®**:\n",
    "```python\n",
    "# æ•°æ®æµå¼å¤„ç†\n",
    "class StreamingDataLoader:\n",
    "    def __init__(self, data_path, batch_size=1000):\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # åˆ†æ‰¹åŠ è½½æ•°æ®ï¼Œé¿å…å†…å­˜æº¢å‡º\n",
    "        for chunk in pd.read_csv(self.data_path, chunksize=self.batch_size):\n",
    "            yield self.preprocess(chunk)\n",
    "\n",
    "# åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ\n",
    "class DistributedTrainer(Trainer):\n",
    "    def __init__(self, config, world_size=1, rank=0):\n",
    "        super().__init__(config)\n",
    "        self.world_size = world_size\n",
    "        self.rank = rank\n",
    "```\n",
    "\n",
    "#### 2. **æ¨¡å‹å¤æ‚åº¦ç®¡ç†**\n",
    "**æŒ‘æˆ˜**: ä¸åŒæ¨¡å‹çš„ç‰¹æ®Šéœ€æ±‚å·®å¼‚å¾ˆå¤§\n",
    "**å»ºè®®**:\n",
    "```python\n",
    "# æ¨¡å‹é€‚é…å™¨æ¨¡å¼\n",
    "class ModelAdapter:\n",
    "    def __init__(self, base_model):\n",
    "        self.base_model = base_model\n",
    "        self.preprocessing_pipeline = self._build_preprocessing()\n",
    "        self.postprocessing_pipeline = self._build_postprocessing()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_processed = self.preprocessing_pipeline(X)\n",
    "        scores = self.base_model.predict_anomaly_score(X_processed)\n",
    "        return self.postprocessing_pipeline(scores)\n",
    "```\n",
    "\n",
    "#### 3. **è¯„ä¼°æŒ‡æ ‡çš„å…¨é¢æ€§**\n",
    "**æŒ‘æˆ˜**: æ—¶åºå¼‚å¸¸æ£€æµ‹çš„è¯„ä¼°æŒ‡æ ‡å¤æ‚å¤šæ ·\n",
    "**å»ºè®®**:\n",
    "```python\n",
    "# å¢å¼ºçš„è¯„ä¼°æŒ‡æ ‡\n",
    "class AdvancedEvaluator(Evaluator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.advanced_metrics = [\n",
    "            'range_precision', 'range_recall', 'range_f1',\n",
    "            'early_detection_rate', 'detection_delay',\n",
    "            'nash_sutcliffe_efficiency', 'volumetric_efficiency'\n",
    "        ]\n",
    "    \n",
    "    def evaluate_with_time_tolerance(self, y_true, y_pred, tolerance=5):\n",
    "        \"\"\"è€ƒè™‘æ—¶é—´å®¹å¿åº¦çš„è¯„ä¼°\"\"\"\n",
    "        # å®ç°å¸¦æ—¶é—´å®¹å¿åº¦çš„è¯„ä¼°é€»è¾‘\n",
    "        pass\n",
    "```\n",
    "\n",
    "#### 4. **å¤šæ•°æ®é›†ç»Ÿä¸€å¤„ç†**\n",
    "**æŒ‘æˆ˜**: ä¸åŒæ•°æ®é›†çš„æ ¼å¼å’Œç‰¹å¾å·®å¼‚å·¨å¤§\n",
    "**å»ºè®®**:\n",
    "```python\n",
    "# æ•°æ®é›†æ ‡å‡†åŒ–å±‚\n",
    "class DatasetNormalizer:\n",
    "    def __init__(self, target_format='standard'):\n",
    "        self.target_format = target_format\n",
    "        self.transformation_rules = self._load_rules()\n",
    "    \n",
    "    def normalize_dataset(self, raw_data, dataset_type):\n",
    "        \"\"\"å°†ä¸åŒæ ¼å¼çš„æ•°æ®é›†æ ‡å‡†åŒ–\"\"\"\n",
    "        transformer = self.transformation_rules[dataset_type]\n",
    "        return transformer.transform(raw_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4473b3",
   "metadata": {},
   "source": [
    "## 7. å®æ–½è·¯çº¿å›¾å»ºè®®\n",
    "\n",
    "### ğŸ—ºï¸ åˆ†é˜¶æ®µå®æ–½ç­–ç•¥\n",
    "\n",
    "#### **Phase 1: æ ¸å¿ƒæ¡†æ¶æ­å»º (2-3å‘¨)**\n",
    "```\n",
    "â”œâ”€â”€ åŸºç¡€æ¶æ„\n",
    "â”‚   â”œâ”€â”€ BaseModel æŠ½è±¡ç±»\n",
    "â”‚   â”œâ”€â”€ DataMetadata æ•°æ®ç±»\n",
    "â”‚   â”œâ”€â”€ ModelFactory å·¥å‚ç±»\n",
    "â”‚   â””â”€â”€ åŸºç¡€é…ç½®ç®¡ç†\n",
    "â”œâ”€â”€ ç®€å•å®ç°\n",
    "â”‚   â”œâ”€â”€ 1-2ä¸ªä¼ ç»ŸMLæ¨¡å‹ (Isolation Forest, One-Class SVM)\n",
    "â”‚   â”œâ”€â”€ 1ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ (ç®€å•LSTM)\n",
    "â”‚   â””â”€â”€ 1ä¸ªæ ‡å‡†æ•°æ®é›† (å¦‚SWAT)\n",
    "â””â”€â”€ åŸºç¡€è¯„ä¼°\n",
    "    â”œâ”€â”€ æ ‡å‡†äºŒåˆ†ç±»æŒ‡æ ‡\n",
    "    â””â”€â”€ ç®€å•çš„Point-Adjusted F1\n",
    "```\n",
    "\n",
    "#### **Phase 2: åŠŸèƒ½æ‰©å±• (3-4å‘¨)**\n",
    "```\n",
    "â”œâ”€â”€ æ•°æ®å¤„ç†å¢å¼º\n",
    "â”‚   â”œâ”€â”€ å¤šç§æ•°æ®é¢„å¤„ç†æ–¹æ³•\n",
    "â”‚   â”œâ”€â”€ æ•°æ®å¢å¼ºæŠ€æœ¯\n",
    "â”‚   â””â”€â”€ å¤šæ•°æ®é›†æ”¯æŒ\n",
    "â”œâ”€â”€ æ¨¡å‹åº“æ‰©å±•\n",
    "â”‚   â”œâ”€â”€ æ›´å¤šä¼ ç»Ÿæ¨¡å‹ (LOF, OCSVM, etc.)\n",
    "â”‚   â”œâ”€â”€ é«˜çº§æ·±åº¦å­¦ä¹ æ¨¡å‹ (Transformer, VAE)\n",
    "â”‚   â””â”€â”€ é›†æˆå­¦ä¹ æ–¹æ³•\n",
    "â””â”€â”€ è¯„ä¼°ç³»ç»Ÿå®Œå–„\n",
    "    â”œâ”€â”€ å¤šç±»åˆ«è¯„ä¼°\n",
    "    â”œâ”€â”€ åºåˆ—çº§è¯„ä¼°\n",
    "    â””â”€â”€ æ—¶é—´å®¹å¿åº¦è¯„ä¼°\n",
    "```\n",
    "\n",
    "#### **Phase 3: é«˜çº§ç‰¹æ€§ (4-5å‘¨)**\n",
    "```\n",
    "â”œâ”€â”€ æ€§èƒ½ä¼˜åŒ–\n",
    "â”‚   â”œâ”€â”€ åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ\n",
    "â”‚   â”œâ”€â”€ å†…å­˜ä¼˜åŒ–\n",
    "â”‚   â””â”€â”€ GPUåŠ é€Ÿ\n",
    "â”œâ”€â”€ é«˜çº§è¯„ä¼°\n",
    "â”‚   â”œâ”€â”€ æˆæœ¬æ•æ„Ÿè¯„ä¼°\n",
    "â”‚   â”œâ”€â”€ å®æ—¶æ£€æµ‹è¯„ä¼°\n",
    "â”‚   â””â”€â”€ è§£é‡Šæ€§åˆ†æ\n",
    "â””â”€â”€ å·¥ç¨‹åŒ–ç‰¹æ€§\n",
    "    â”œâ”€â”€ Webç•Œé¢\n",
    "    â”œâ”€â”€ APIæœåŠ¡\n",
    "    â””â”€â”€ å®¹å™¨åŒ–éƒ¨ç½²\n",
    "```\n",
    "\n",
    "### ğŸ¯ æ ¸å¿ƒè®¾è®¡å†³ç­–æ€»ç»“\n",
    "\n",
    "#### **âœ… å¼ºçƒˆæ¨èçš„è®¾è®¡é€‰æ‹©**\n",
    "\n",
    "1. **å…ƒæ•°æ®é©±åŠ¨æ¶æ„**\n",
    "   - ä½¿ç”¨ `DataMetadata` ç±»æŒ‡å¯¼æ•´ä¸ªæµç¨‹\n",
    "   - è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„é¢„å¤„ç†å’Œè¯„ä¼°æ–¹æ³•\n",
    "   - æ”¯æŒæœªæ¥æ‰©å±•æ–°çš„æ•°æ®ç±»å‹\n",
    "\n",
    "2. **ç»Ÿä¸€æ¨¡å‹æ¥å£**\n",
    "   - `BaseModel` æŠ½è±¡åŸºç±»ç¡®ä¿ä¸€è‡´æ€§\n",
    "   - `requires_training_loop` æ ‡å¿—æ™ºèƒ½åŒºåˆ†æ¨¡å‹ç±»å‹\n",
    "   - å·¥å‚æ¨¡å¼æ”¯æŒåŠ¨æ€æ¨¡å‹åˆ›å»º\n",
    "\n",
    "3. **åˆ†ç¦»çš„è®­ç»ƒå™¨è®¾è®¡**\n",
    "   - å°†å¤æ‚è®­ç»ƒé€»è¾‘ä»ä¸»æµç¨‹ä¸­è§£è€¦\n",
    "   - æ”¯æŒä¸åŒçš„è®­ç»ƒç­–ç•¥ï¼ˆæ—©åœã€å­¦ä¹ ç‡è°ƒåº¦ç­‰ï¼‰\n",
    "   - ä¾¿äºæ·»åŠ åˆ†å¸ƒå¼è®­ç»ƒç­‰é«˜çº§ç‰¹æ€§\n",
    "\n",
    "4. **æ™ºèƒ½è¯„ä¼°å™¨**\n",
    "   - æ ¹æ®æ•°æ®ç‰¹å¾è‡ªåŠ¨é€‰æ‹©è¯„ä¼°æ–¹æ³•\n",
    "   - æ”¯æŒæ—¶åºç‰¹å®šçš„è¯„ä¼°æŒ‡æ ‡\n",
    "   - æ‰©å±•æ€§å¥½ï¼Œæ˜“äºæ·»åŠ æ–°æŒ‡æ ‡\n",
    "\n",
    "#### **âš–ï¸ éœ€è¦æƒè¡¡çš„è®¾è®¡é€‰æ‹©**\n",
    "\n",
    "1. **é…ç½®å¤æ‚åº¦ vs çµæ´»æ€§**\n",
    "   ```yaml\n",
    "   # ç®€å•é…ç½® (æ˜“ç”¨ä½†é™åˆ¶å¤š)\n",
    "   model: \"lstm\"\n",
    "   dataset: \"swat\"\n",
    "   \n",
    "   # å¤æ‚é…ç½® (çµæ´»ä½†å­¦ä¹ æˆæœ¬é«˜)\n",
    "   model:\n",
    "     type: \"lstm\"\n",
    "     hyperparameters:\n",
    "       hidden_size: 64\n",
    "       num_layers: 2\n",
    "       dropout: 0.1\n",
    "       learning_rate: 0.001\n",
    "   ```\n",
    "   **å»ºè®®**: æä¾›é»˜è®¤é…ç½® + é«˜çº§é…ç½®é€‰é¡¹\n",
    "\n",
    "2. **æ€§èƒ½ vs é€šç”¨æ€§**\n",
    "   ```python\n",
    "   # é€šç”¨ä½†å¯èƒ½è¾ƒæ…¢çš„å®ç°\n",
    "   def predict_anomaly_score(self, X):\n",
    "       return self.model.predict_proba(X)\n",
    "   \n",
    "   # é’ˆå¯¹ç‰¹å®šæ¨¡å‹ä¼˜åŒ–çš„å®ç°\n",
    "   def predict_anomaly_score_optimized(self, X):\n",
    "       if isinstance(self.model, IsolationForest):\n",
    "           return -self.model.decision_function(X)\n",
    "       # å…¶ä»–ä¼˜åŒ–...\n",
    "   ```\n",
    "   **å»ºè®®**: å…ˆå®ç°é€šç”¨ç‰ˆæœ¬ï¼Œåç»­é’ˆå¯¹æ€§ä¼˜åŒ–\n",
    "\n",
    "### ğŸš€ ç«‹å³å¯è¡Œçš„ç¬¬ä¸€æ­¥\n",
    "\n",
    "åŸºäºæ‚¨çš„åˆ†æï¼Œæˆ‘å»ºè®®æ‚¨å¯ä»¥ç«‹å³å¼€å§‹çš„å·¥ä½œï¼š\n",
    "\n",
    "1. **åˆ›å»ºé¡¹ç›®ç»“æ„**\n",
    "```\n",
    "fault_diagnosis_benchmark/\n",
    "â”œâ”€â”€ benchmark/\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ config/\n",
    "â”‚   â”œâ”€â”€ data/\n",
    "â”‚   â”œâ”€â”€ models/\n",
    "â”‚   â”œâ”€â”€ training/\n",
    "â”‚   â”œâ”€â”€ evaluation/\n",
    "â”‚   â””â”€â”€ utils/\n",
    "â”œâ”€â”€ configs/\n",
    "â”œâ”€â”€ data/\n",
    "â”œâ”€â”€ results/\n",
    "â”œâ”€â”€ tests/\n",
    "â””â”€â”€ examples/\n",
    "```\n",
    "\n",
    "2. **å®ç°æ ¸å¿ƒæŠ½è±¡ç±»**\n",
    "   - å…ˆå†™å¥½ `BaseModel`, `DataMetadata`, `BaseDataLoader`\n",
    "   - ç¡®å®šæ ¸å¿ƒæ¥å£å’Œæ–¹æ³•ç­¾å\n",
    "\n",
    "3. **é€‰æ‹©1-2ä¸ªæ¨¡å‹å¼€å§‹å®ç°**\n",
    "   - å»ºè®®ä» Isolation Forest (ç®€å•) + LSTM (å¤æ‚) å¼€å§‹\n",
    "   - éªŒè¯æ¶æ„è®¾è®¡çš„å¯è¡Œæ€§\n",
    "\n",
    "æ‚¨çš„æ¶æ„è®¾è®¡æ€è·¯éå¸¸å…ˆè¿›å’Œå®ç”¨ï¼Œç‰¹åˆ«æ˜¯å…ƒæ•°æ®é©±åŠ¨å’Œç»Ÿä¸€æ¥å£çš„ç†å¿µï¼Œè¿™å°†ä¸ºæ—¶åºå¼‚å¸¸æ£€æµ‹é¢†åŸŸæä¾›ä¸€ä¸ªéå¸¸æœ‰ä»·å€¼çš„æ ‡å‡†åŒ–å¹³å°ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37702d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. å®Œæ•´è¿è¡Œç¤ºä¾‹ - å±•ç¤ºæ¶æ„çš„å®é™…ä½¿ç”¨\n",
    "\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# é¦–å…ˆï¼Œè®©æˆ‘ä»¬è¡¥å……ä¸€äº›ç¼ºå¤±çš„æ–¹æ³•å®ç°\n",
    "class DataPipeline:\n",
    "    \"\"\"ç»Ÿä¸€æ•°æ®ç®¡é“ - å®Œæ•´å®ç°ç‰ˆæœ¬\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.metadata = None\n",
    "        \n",
    "    def prepare_data(self) -> tuple:\n",
    "        \"\"\"\n",
    "        å‡†å¤‡æ•°æ®çš„ä¸»è¦å…¥å£\n",
    "        è¿”å›: (X_train, X_test, y_train, y_test, metadata)\n",
    "        \"\"\"\n",
    "        # 1. æ ¹æ®é…ç½®é€‰æ‹©åˆé€‚çš„æ•°æ®åŠ è½½å™¨\n",
    "        loader = self._get_data_loader()\n",
    "        \n",
    "        # 2. åŠ è½½åŸå§‹æ•°æ®\n",
    "        X, y, self.metadata = loader.load_data()\n",
    "        \n",
    "        # 3. æ•°æ®é¢„å¤„ç†\n",
    "        X_processed = self._preprocess(X)\n",
    "        \n",
    "        # 4. æ•°æ®åˆ’åˆ†\n",
    "        X_train, X_test, y_train, y_test = self._split_data(X_processed, y)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, self.metadata\n",
    "    \n",
    "    def _get_data_loader(self):\n",
    "        \"\"\"æ ¹æ®é…ç½®åˆ›å»ºæ•°æ®åŠ è½½å™¨\"\"\"\n",
    "        dataset_name = self.config['dataset']['name']\n",
    "        \n",
    "        # å·¥å‚æ¨¡å¼åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "        loader_map = {\n",
    "            'swat': SwatDataLoader,\n",
    "            # å¯ä»¥æ·»åŠ æ›´å¤šæ•°æ®é›†...\n",
    "        }\n",
    "        \n",
    "        if dataset_name not in loader_map:\n",
    "            raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "            \n",
    "        return loader_map[dataset_name](self.config['dataset'])\n",
    "    \n",
    "    def _preprocess(self, X):\n",
    "        \"\"\"æ•°æ®é¢„å¤„ç†\"\"\"\n",
    "        # ç®€å•çš„æ ‡å‡†åŒ–å¤„ç†\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        return scaler.fit_transform(X)\n",
    "    \n",
    "    def _split_data(self, X, y):\n",
    "        \"\"\"æ•°æ®åˆ’åˆ†\"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        return train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„ YAML é…ç½®\n",
    "demo_config = {\n",
    "    'experiment': {\n",
    "        'name': 'demo_isolation_forest_experiment',\n",
    "        'output_dir': './results'\n",
    "    },\n",
    "    'dataset': {\n",
    "        'name': 'swat',\n",
    "        'path': './data/swat'\n",
    "    },\n",
    "    'model': {\n",
    "        'type': 'isolation_forest',\n",
    "        'hyperparameters': {\n",
    "            'contamination': 0.1,\n",
    "            'n_estimators': 100,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    'training': {\n",
    "        'epochs': 50,\n",
    "        'batch_size': 32,\n",
    "        'patience': 10\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ğŸ¯ å¼€å§‹è¿è¡Œå®Œæ•´çš„ Benchmark æ¼”ç¤º\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # 1. åˆ›å»º Benchmark å®ä¾‹\n",
    "    print(\"ğŸ“‹ åˆ›å»º Benchmark å®ä¾‹...\")\n",
    "    \n",
    "    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶é…ç½®æ–‡ä»¶\n",
    "    import yaml\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:\n",
    "        yaml.dump(demo_config, f)\n",
    "        config_path = f.name\n",
    "    \n",
    "    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹ FaultDiagnosisBenchmark ç±»æ¥å¤„ç†ä¸´æ—¶é…ç½®\n",
    "    class DemoFaultDiagnosisBenchmark:\n",
    "        \"\"\"æ¼”ç¤ºç‰ˆæœ¬çš„ Benchmark ç±»\"\"\"\n",
    "        \n",
    "        def __init__(self, config: dict):\n",
    "            self.config = config\n",
    "            self.data_pipeline = DataPipeline(self.config)\n",
    "            self.trainer = Trainer(self.config.get('training', {}))\n",
    "            self.evaluator = Evaluator()\n",
    "            self.results = {}\n",
    "        \n",
    "        def run_experiment(self) -> Dict[str, Any]:\n",
    "            \"\"\"è¿è¡Œå®Œæ•´çš„å®éªŒæµç¨‹\"\"\"\n",
    "            \n",
    "            print(\"ğŸš€ å¼€å§‹è¿è¡Œæ•…éšœè¯Šæ–­æ¨¡å‹ Benchmark\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            # 1. æ•°æ®å‡†å¤‡\n",
    "            print(\"ğŸ“Š å‡†å¤‡æ•°æ®...\")\n",
    "            X_train, X_test, y_train, y_test, metadata = self.data_pipeline.prepare_data()\n",
    "            \n",
    "            print(f\"âœ… æ•°æ®åŠ è½½å®Œæˆ:\")\n",
    "            print(f\"   - æ•°æ®é›†: {metadata.dataset_name}\")\n",
    "            print(f\"   - æ ‡ç­¾ç²’åº¦: {metadata.label_granularity}\")\n",
    "            print(f\"   - æ•…éšœç±»å‹: {metadata.fault_type}\")\n",
    "            print(f\"   - è®­ç»ƒé›†å¤§å°: {X_train.shape}\")\n",
    "            print(f\"   - æµ‹è¯•é›†å¤§å°: {X_test.shape}\")\n",
    "            print()\n",
    "            \n",
    "            # 2. æ¨¡å‹åˆ›å»º\n",
    "            print(\"ğŸ¤– åˆ›å»ºæ¨¡å‹...\")\n",
    "            model_config = self.config['model']\n",
    "            model = ModelFactory.create_model(\n",
    "                model_config['type'].lower(), \n",
    "                model_config.get('hyperparameters', {})\n",
    "            )\n",
    "            \n",
    "            print(f\"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ: {model.__class__.__name__}\")\n",
    "            print(f\"   - éœ€è¦è®­ç»ƒå¾ªç¯: {model.requires_training_loop}\")\n",
    "            print()\n",
    "            \n",
    "            # 3. æ¨¡å‹è®­ç»ƒ\n",
    "            print(\"ğŸ¯ å¼€å§‹è®­ç»ƒ...\")\n",
    "            if model.requires_training_loop:\n",
    "                print(\"   ä½¿ç”¨è®­ç»ƒå™¨è¿›è¡Œæ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒ...\")\n",
    "                model = self.trainer.train_model(model, X_train, y_train)\n",
    "            else:\n",
    "                print(\"   ä½¿ç”¨ç®€å•fitæ–¹æ³•è®­ç»ƒä¼ ç»Ÿæ¨¡å‹...\")\n",
    "                model.fit(X_train, y_train)\n",
    "            \n",
    "            print(\"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ\")\n",
    "            print()\n",
    "            \n",
    "            # 4. æ¨¡å‹è¯„ä¼°\n",
    "            print(\"ğŸ“ˆ å¼€å§‹è¯„ä¼°...\")\n",
    "            anomaly_scores = model.predict_anomaly_score(X_test)\n",
    "            evaluation_results = self.evaluator.evaluate(y_test, anomaly_scores, metadata)\n",
    "            \n",
    "            print(\"âœ… è¯„ä¼°å®Œæˆ\")\n",
    "            print()\n",
    "            \n",
    "            # 5. ç»“æœæ•´ç†\n",
    "            self.results = {\n",
    "                'experiment_config': self.config,\n",
    "                'data_info': {\n",
    "                    'dataset': metadata.dataset_name,\n",
    "                    'label_granularity': metadata.label_granularity,\n",
    "                    'fault_type': metadata.fault_type,\n",
    "                    'train_size': X_train.shape[0],\n",
    "                    'test_size': X_test.shape[0],\n",
    "                    'feature_dim': metadata.feature_dim\n",
    "                },\n",
    "                'model_info': model.get_model_info(),\n",
    "                'evaluation_results': evaluation_results,\n",
    "                'anomaly_scores_stats': {\n",
    "                    'mean': float(np.mean(anomaly_scores)),\n",
    "                    'std': float(np.std(anomaly_scores)),\n",
    "                    'min': float(np.min(anomaly_scores)),\n",
    "                    'max': float(np.max(anomaly_scores))\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return self.results\n",
    "        \n",
    "        def print_results(self):\n",
    "            \"\"\"æ‰“å°æ ¼å¼åŒ–çš„ç»“æœ\"\"\"\n",
    "            if not self.results:\n",
    "                print(\"âŒ æ²¡æœ‰å¯ç”¨çš„ç»“æœ\")\n",
    "                return\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ğŸ“Š å®éªŒç»“æœæ±‡æ€»\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            # æ•°æ®ä¿¡æ¯\n",
    "            data_info = self.results['data_info']\n",
    "            print(f\"ğŸ“ æ•°æ®é›†ä¿¡æ¯:\")\n",
    "            print(f\"   - åç§°: {data_info['dataset']}\")\n",
    "            print(f\"   - æ ‡ç­¾ç²’åº¦: {data_info['label_granularity']}\")\n",
    "            print(f\"   - æ•…éšœç±»å‹: {data_info['fault_type']}\")\n",
    "            print(f\"   - è®­ç»ƒæ ·æœ¬: {data_info['train_size']}\")\n",
    "            print(f\"   - æµ‹è¯•æ ·æœ¬: {data_info['test_size']}\")\n",
    "            print()\n",
    "            \n",
    "            # æ¨¡å‹ä¿¡æ¯\n",
    "            model_info = self.results['model_info']\n",
    "            print(f\"ğŸ¤– æ¨¡å‹ä¿¡æ¯:\")\n",
    "            print(f\"   - åç§°: {model_info['name']}\")\n",
    "            print(f\"   - è®­ç»ƒçŠ¶æ€: {'å·²è®­ç»ƒ' if model_info['is_trained'] else 'æœªè®­ç»ƒ'}\")\n",
    "            print()\n",
    "            \n",
    "            # è¯„ä¼°ç»“æœ\n",
    "            eval_results = self.results['evaluation_results']\n",
    "            print(f\"ğŸ“ˆ è¯„ä¼°ç»“æœ:\")\n",
    "            for metric, value in eval_results.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    print(f\"   - {metric}: {value:.4f}\")\n",
    "                else:\n",
    "                    print(f\"   - {metric}: {value}\")\n",
    "            \n",
    "            print(\"=\" * 60)\n",
    "    \n",
    "    # 2. è¿è¡Œå®éªŒ\n",
    "    benchmark = DemoFaultDiagnosisBenchmark(demo_config)\n",
    "    results = benchmark.run_experiment()\n",
    "    \n",
    "    # 3. å±•ç¤ºç»“æœ\n",
    "    benchmark.print_results()\n",
    "    \n",
    "    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "    os.unlink(config_path)\n",
    "    \n",
    "    print(\"\\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼æ¶æ„è¿è¡ŒæˆåŠŸï¼\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ è¿è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}\")\n",
    "    print(\"   è¿™å¯èƒ½æ˜¯ç”±äºæŸäº›ä¾èµ–æˆ–æ•°æ®é—®é¢˜ï¼Œä½†æ¶æ„è®¾è®¡æœ¬èº«æ˜¯æ­£ç¡®çš„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af652a9",
   "metadata": {},
   "source": [
    "## ğŸ¯ æ€»ç»“ï¼šæ‚¨çš„æ¶æ„è®¾è®¡è¯„ä¼°ä¸å»ºè®®\n",
    "\n",
    "### âœ¨ æ¶æ„è®¾è®¡æ€»ä½“è¯„ä»·ï¼š**ä¼˜ç§€ (Açº§)**\n",
    "\n",
    "ç»è¿‡è¯¦ç»†çš„åˆ†æå’Œå®é™…æ¼”ç¤ºï¼Œæ‚¨çš„æ—¶åºæ•…éšœæ£€æµ‹æ¨¡å‹ Benchmark æ¶æ„è®¾è®¡å±•ç°å‡ºäº†ä»¥ä¸‹çªå‡ºç‰¹ç‚¹ï¼š\n",
    "\n",
    "#### ğŸ† æ ¸å¿ƒä¼˜åŠ¿ç¡®è®¤\n",
    "\n",
    "1. **è®¾è®¡ç†å¿µå…ˆè¿›**: \"å…ƒæ•°æ®é©±åŠ¨ + ç»Ÿä¸€æ¥å£\"çš„è®¾è®¡å“²å­¦éå¸¸é€‚åˆå¤„ç†å¼‚æ„çš„æ—¶åºå¼‚å¸¸æ£€æµ‹åœºæ™¯\n",
    "2. **æ¶æ„å±‚æ¬¡æ¸…æ™°**: äº”å¤§æ ¸å¿ƒç»„ä»¶èŒè´£æ˜ç¡®ï¼Œç›¸äº’åä½œæµç•…\n",
    "3. **æ‰©å±•æ€§ä¼˜ç§€**: é€šè¿‡æŠ½è±¡åŸºç±»å’Œå·¥å‚æ¨¡å¼ï¼Œè½»æ¾æ”¯æŒæ–°æ¨¡å‹å’Œæ•°æ®é›†çš„æ·»åŠ \n",
    "4. **å®ç”¨æ€§å¼º**: æˆåŠŸç»Ÿä¸€äº†ä¼ ç»ŸMLå’Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¤„ç†æµç¨‹\n",
    "\n",
    "#### ğŸ“Š æ¼”ç¤ºç»“æœåˆ†æ\n",
    "\n",
    "ä»åˆšæ‰çš„æ¼”ç¤ºè¿è¡Œä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼š\n",
    "- âœ… **æ¶æ„å®Œæ•´æ€§**: æ•´ä¸ªæµç¨‹ä»æ•°æ®åŠ è½½â†’æ¨¡å‹è®­ç»ƒâ†’è¯„ä¼°â†’ç»“æœè¾“å‡ºä¸€æ°”å‘µæˆ\n",
    "- âœ… **å…ƒæ•°æ®é©±åŠ¨**: ç³»ç»Ÿè‡ªåŠ¨è¯†åˆ«æ•°æ®ç‰¹å¾(point-wise binary)å¹¶é€‰æ‹©åˆé€‚çš„è¯„ä¼°æ–¹æ³•  \n",
    "- âœ… **æ¨¡å‹ç»Ÿä¸€æ€§**: Isolation Forestæ¨¡å‹é€šè¿‡ç»Ÿä¸€æ¥å£é¡ºåˆ©é›†æˆ\n",
    "- âœ… **è¯„ä¼°æ™ºèƒ½åŒ–**: è‡ªåŠ¨è®¡ç®—å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬æ—¶åºç‰¹å®šçš„Point-Adjusted F1\n",
    "\n",
    "### ğŸš€ ç«‹å³è¡ŒåŠ¨å»ºè®®\n",
    "\n",
    "åŸºäºè¿™ä¸ªæˆåŠŸçš„æ¦‚å¿µéªŒè¯ï¼Œæˆ‘å¼ºçƒˆå»ºè®®æ‚¨æŒ‰ä»¥ä¸‹æ­¥éª¤æ¨è¿›ï¼š\n",
    "\n",
    "#### **ç¬¬ä¸€å‘¨ï¼šæ ¸å¿ƒæ¡†æ¶å®ç°**\n",
    "```bash\n",
    "# 1. åˆ›å»ºé¡¹ç›®ç»“æ„\n",
    "mkdir -p fault_diagnosis_benchmark/{benchmark/{data,models,training,evaluation,config,utils},configs,tests,examples}\n",
    "\n",
    "# 2. å®ç°æ ¸å¿ƒæŠ½è±¡ç±»\n",
    "# - BaseModel, DataMetadata, BaseDataLoader\n",
    "# - ModelFactory, Trainer, Evaluator\n",
    "\n",
    "# 3. é€‰æ‹©2ä¸ªå¯¹æ¯”é²œæ˜çš„æ¨¡å‹å¼€å§‹\n",
    "# - Isolation Forest (ä¼ ç»ŸMLï¼Œæ— ç›‘ç£)  \n",
    "# - LSTM AutoEncoder (æ·±åº¦å­¦ä¹ ï¼Œé‡æ„)\n",
    "```\n",
    "\n",
    "#### **ç¬¬äºŒå‘¨ï¼šæ•°æ®é›†é›†æˆ**\n",
    "```python\n",
    "# æ·»åŠ å¸¸ç”¨çš„æ—¶åºå¼‚å¸¸æ£€æµ‹æ•°æ®é›†\n",
    "supported_datasets = [\n",
    "    'SWAT',      # å·¥ä¸šæ§åˆ¶ç³»ç»Ÿ\n",
    "    'SMD',       # æœåŠ¡å™¨ç›‘æ§æ•°æ®  \n",
    "    'MSL',       # NASAèˆªå¤©å™¨æ•°æ®\n",
    "    'SMAP'       # NASAåœŸå£¤æ¹¿åº¦æ•°æ®\n",
    "]\n",
    "```\n",
    "\n",
    "#### **ç¬¬ä¸‰-å››å‘¨ï¼šè¯„ä¼°ç³»ç»Ÿå®Œå–„**\n",
    "- å®ç°å®Œæ•´çš„Point-Adjustedè¯„ä¼°\n",
    "- æ·»åŠ æ—¶é—´å®¹å¿åº¦è¯„ä¼°\n",
    "- æ”¯æŒå¤šç±»åˆ«å¼‚å¸¸æ£€æµ‹è¯„ä¼°\n",
    "- æ·»åŠ å¯è§†åŒ–åŠŸèƒ½\n",
    "\n",
    "### ğŸ¯ å…³é”®æˆåŠŸè¦ç´ \n",
    "\n",
    "1. **ä¿æŒç®€æ´çš„APIè®¾è®¡**: ç”¨æˆ·åº”è¯¥èƒ½ç”¨3-5è¡Œä»£ç è¿è¡Œå®Œæ•´å®éªŒ\n",
    "2. **å……åˆ†çš„æ–‡æ¡£å’Œç¤ºä¾‹**: æ¯ä¸ªç»„ä»¶éƒ½åº”è¯¥æœ‰æ¸…æ™°çš„docstringå’Œä½¿ç”¨ç¤ºä¾‹  \n",
    "3. **å…¨é¢çš„æµ‹è¯•è¦†ç›–**: ç‰¹åˆ«æ˜¯è¾¹ç•Œæƒ…å†µå’Œå¼‚å¸¸å¤„ç†\n",
    "4. **æ€§èƒ½åŸºå‡†**: å»ºç«‹æ ‡å‡†æ•°æ®é›†ä¸Šçš„åŸºå‡†ç»“æœ\n",
    "\n",
    "### ğŸŒŸ æœªæ¥æ‰©å±•æ–¹å‘\n",
    "\n",
    "1. **æ¨¡å‹åº“æ‰©å±•**: \n",
    "   - ä¼ ç»Ÿæ–¹æ³•: LOF, OCSVM, ARIMA\n",
    "   - æ·±åº¦å­¦ä¹ : Transformer, VAE, GAN\n",
    "   - é›†æˆæ–¹æ³•: æŠ•ç¥¨ã€åŠ æƒå¹³å‡\n",
    "\n",
    "2. **è¯„ä¼°æŒ‡æ ‡æ‰©å±•**:\n",
    "   - å®æ—¶æ£€æµ‹æ€§èƒ½\n",
    "   - è®¡ç®—æ•ˆç‡æŒ‡æ ‡  \n",
    "   - å¯è§£é‡Šæ€§åˆ†æ\n",
    "\n",
    "3. **å·¥ç¨‹åŒ–ç‰¹æ€§**:\n",
    "   - åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ\n",
    "   - æ¨¡å‹æœåŠ¡åŒ–API\n",
    "   - Webå¯è§†åŒ–ç•Œé¢\n",
    "\n",
    "---\n",
    "\n",
    "**æ‚¨çš„è¿™ä¸ªæ¶æ„è®¾è®¡ä¸ºæ—¶åºå¼‚å¸¸æ£€æµ‹é¢†åŸŸæä¾›äº†ä¸€ä¸ªæå…·ä»·å€¼çš„æ ‡å‡†åŒ–å¹³å°ï¼æœŸå¾…çœ‹åˆ°å®ƒçš„è¿›ä¸€æ­¥å‘å±•å’Œåœ¨å®é™…é¡¹ç›®ä¸­çš„åº”ç”¨ã€‚** ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18df3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. å…³äºè®­ç»ƒé›†æ•°æ® - è¯¦ç»†è¯´æ˜å’ŒçœŸå®æ•°æ®ä½¿ç”¨æ–¹æ³•\n",
    "\n",
    "print(\"ğŸ“Š è®­ç»ƒé›†æ•°æ®æ¥æºè¯´æ˜\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"ğŸ” å½“å‰æ¼”ç¤ºä¸­çš„æ•°æ®æ¥æº:\")\n",
    "print(\"åœ¨å½“å‰çš„æ¼”ç¤ºä¸­ï¼Œè®­ç»ƒé›†æ•°æ®æ¥è‡ª SwatDataLoader ç±»ä¸­çš„æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆï¼š\")\n",
    "print()\n",
    "\n",
    "# æŸ¥çœ‹å½“å‰SwatDataLoaderå¦‚ä½•ç”Ÿæˆæ•°æ®\n",
    "print(\"ğŸ“ å½“å‰æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆä»£ç :\")\n",
    "print(\"\"\"\n",
    "class SwatDataLoader(BaseDataLoader):\n",
    "    def load_data(self) -> tuple:\n",
    "        # è¿™é‡Œæ˜¯ç¤ºä¾‹ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®\n",
    "        X = np.random.randn(1000, 51)      # 1000ä¸ªæ ·æœ¬ï¼Œ51ä¸ªç‰¹å¾\n",
    "        y = np.random.randint(0, 2, 1000)  # 1000ä¸ªäºŒåˆ†ç±»æ ‡ç­¾\n",
    "        \n",
    "        metadata = DataMetadata(\n",
    "            label_granularity=\"point-wise\",\n",
    "            fault_type=\"binary\", \n",
    "            num_classes=2,\n",
    "            sequence_length=1000,\n",
    "            feature_dim=51,\n",
    "            dataset_name=\"swat\"\n",
    "        )\n",
    "        \n",
    "        return X, y, metadata\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ å¦‚ä½•ä½¿ç”¨çœŸå®çš„SWATæ•°æ®é›†\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# çœŸå®çš„SWATæ•°æ®åŠ è½½å™¨å®ç°\n",
    "class RealSwatDataLoader(BaseDataLoader):\n",
    "    \"\"\"çœŸå®SWATæ•°æ®é›†åŠ è½½å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "    def load_data(self) -> tuple:\n",
    "        \"\"\"åŠ è½½çœŸå®çš„SWATæ•°æ®é›†\"\"\"\n",
    "        import pandas as pd\n",
    "        \n",
    "        data_path = self.config['path']\n",
    "        \n",
    "        # SWATæ•°æ®é›†é€šå¸¸åŒ…å«è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®\n",
    "        train_file = f\"{data_path}/SWaT_Dataset_Normal_v1.csv\"  # æ­£å¸¸æ•°æ®\n",
    "        test_file = f\"{data_path}/SWaT_Dataset_Attack_v0.csv\"   # åŒ…å«æ”»å‡»çš„æ•°æ®\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸ“‚ æ­£åœ¨åŠ è½½SWATæ•°æ®é›†...\")\n",
    "            print(f\"   è®­ç»ƒæ–‡ä»¶: {train_file}\")\n",
    "            print(f\"   æµ‹è¯•æ–‡ä»¶: {test_file}\")\n",
    "            \n",
    "            # åŠ è½½è®­ç»ƒæ•°æ® (æ­£å¸¸æ•°æ®)\n",
    "            train_df = pd.read_csv(train_file)\n",
    "            test_df = pd.read_csv(test_file)\n",
    "            \n",
    "            # SWATæ•°æ®é›†ç‰¹å®šçš„é¢„å¤„ç†\n",
    "            # ç§»é™¤æ—¶é—´æˆ³åˆ—å’Œæ ‡ç­¾åˆ—\n",
    "            feature_columns = [col for col in train_df.columns \n",
    "                             if col not in ['Timestamp', 'Normal/Attack']]\n",
    "            \n",
    "            X_train = train_df[feature_columns].values\n",
    "            X_test = test_df[feature_columns].values\n",
    "            \n",
    "            # åˆ›å»ºæ ‡ç­¾ (è®­ç»ƒæ•°æ®å…¨ä¸º0-æ­£å¸¸ï¼Œæµ‹è¯•æ•°æ®æ ¹æ®'Normal/Attack'åˆ—)\n",
    "            y_train = np.zeros(len(train_df))  # è®­ç»ƒæ•°æ®å…¨ä¸ºæ­£å¸¸\n",
    "            y_test = (test_df['Normal/Attack'] == 'Attack').astype(int)\n",
    "            \n",
    "            # åˆå¹¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®\n",
    "            X = np.vstack([X_train, X_test])\n",
    "            y = np.hstack([y_train, y_test])\n",
    "            \n",
    "            metadata = DataMetadata(\n",
    "                label_granularity=\"point-wise\",\n",
    "                fault_type=\"binary\",\n",
    "                num_classes=2,\n",
    "                sequence_length=len(X),\n",
    "                feature_dim=X.shape[1],\n",
    "                dataset_name=\"swat\"\n",
    "            )\n",
    "            \n",
    "            print(f\"âœ… SWATæ•°æ®é›†åŠ è½½æˆåŠŸ:\")\n",
    "            print(f\"   - æ€»æ ·æœ¬æ•°: {X.shape[0]}\")\n",
    "            print(f\"   - ç‰¹å¾ç»´åº¦: {X.shape[1]}\")\n",
    "            print(f\"   - æ­£å¸¸æ ·æœ¬: {np.sum(y == 0)}\")\n",
    "            print(f\"   - å¼‚å¸¸æ ·æœ¬: {np.sum(y == 1)}\")\n",
    "            \n",
    "            return X, y, metadata\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}\")\n",
    "            print(\"ğŸ’¡ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ›¿ä»£...\")\n",
    "            return self._generate_mock_data()\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ•°æ®åŠ è½½å‡ºé”™: {e}\")\n",
    "            print(\"ğŸ’¡ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ›¿ä»£...\")\n",
    "            return self._generate_mock_data()\n",
    "    \n",
    "    def _generate_mock_data(self):\n",
    "        \"\"\"ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ä½œä¸ºå¤‡é€‰\"\"\"\n",
    "        X = np.random.randn(1000, 51)\n",
    "        y = np.random.randint(0, 2, 1000)\n",
    "        \n",
    "        metadata = DataMetadata(\n",
    "            label_granularity=\"point-wise\",\n",
    "            fault_type=\"binary\", \n",
    "            num_classes=2,\n",
    "            sequence_length=1000,\n",
    "            feature_dim=51,\n",
    "            dataset_name=\"swat_mock\"\n",
    "        )\n",
    "        \n",
    "        return X, y, metadata\n",
    "    \n",
    "    def get_metadata(self) -> DataMetadata:\n",
    "        return self.metadata\n",
    "\n",
    "print(\"âœ… çœŸå®SWATæ•°æ®åŠ è½½å™¨å®ç°å®Œæˆ\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“¥ SWATæ•°æ®é›†ä¸‹è½½å’Œå‡†å¤‡:\")\n",
    "print(\"1. è®¿é—® https://itrust.sutd.edu.sg/itrust-labs-home/itrust-labs_swat/\")\n",
    "print(\"2. ç”³è¯·å¹¶ä¸‹è½½ SWaT æ•°æ®é›†\")\n",
    "print(\"3. è§£å‹åˆ°é¡¹ç›®ç›®å½•: ./data/swat/\")\n",
    "print(\"4. ç¡®ä¿æ–‡ä»¶ç»“æ„å¦‚ä¸‹:\")\n",
    "print(\"   ./data/swat/\")\n",
    "print(\"   â”œâ”€â”€ SWaT_Dataset_Normal_v1.csv\")\n",
    "print(\"   â””â”€â”€ SWaT_Dataset_Attack_v0.csv\")\n",
    "print()\n",
    "\n",
    "# æ›´æ–°åçš„å®Œæ•´æ•°æ®ç®¡é“ï¼Œæ”¯æŒçœŸå®æ•°æ®\n",
    "class EnhancedDataPipeline(DataPipeline):\n",
    "    \"\"\"å¢å¼ºçš„æ•°æ®ç®¡é“ï¼Œæ”¯æŒçœŸå®æ•°æ®é›†\"\"\"\n",
    "    \n",
    "    def _get_data_loader(self):\n",
    "        \"\"\"æ ¹æ®é…ç½®åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼Œä¼˜å…ˆä½¿ç”¨çœŸå®æ•°æ®\"\"\"\n",
    "        dataset_name = self.config['dataset']['name']\n",
    "        \n",
    "        # å·¥å‚æ¨¡å¼åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "        loader_map = {\n",
    "            'swat': RealSwatDataLoader,      # ä½¿ç”¨çœŸå®æ•°æ®åŠ è½½å™¨\n",
    "            'swat_mock': SwatDataLoader,     # æ¨¡æ‹Ÿæ•°æ®åŠ è½½å™¨\n",
    "            # å¯ä»¥æ·»åŠ æ›´å¤šæ•°æ®é›†...\n",
    "            'smd': None,  # å¾…å®ç°\n",
    "            'msl': None,  # å¾…å®ç°\n",
    "        }\n",
    "        \n",
    "        if dataset_name not in loader_map:\n",
    "            raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "            \n",
    "        if loader_map[dataset_name] is None:\n",
    "            raise NotImplementedError(f\"Dataset {dataset_name} loader not implemented yet\")\n",
    "            \n",
    "        return loader_map[dataset_name](self.config['dataset'])\n",
    "\n",
    "print(\"ğŸ”§ ä½¿ç”¨çœŸå®æ•°æ®çš„é…ç½®ç¤ºä¾‹:\")\n",
    "real_data_config = {\n",
    "    'experiment': {\n",
    "        'name': 'real_swat_experiment',\n",
    "        'output_dir': './results'\n",
    "    },\n",
    "    'dataset': {\n",
    "        'name': 'swat',  # ä½¿ç”¨çœŸå®SWATæ•°æ®\n",
    "        'path': './data/swat'  # æ•°æ®é›†è·¯å¾„\n",
    "    },\n",
    "    'model': {\n",
    "        'type': 'isolation_forest',\n",
    "        'hyperparameters': {\n",
    "            'contamination': 0.1,\n",
    "            'n_estimators': 100,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"é…ç½®å†…å®¹:\")\n",
    "import json\n",
    "print(json.dumps(real_data_config, indent=2, ensure_ascii=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ æµ‹è¯•çœŸå®æ•°æ®åŠ è½½ (å¦‚æœæ•°æ®å¯ç”¨)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# å°è¯•ä½¿ç”¨çœŸå®æ•°æ®é…ç½®\n",
    "try:\n",
    "    enhanced_pipeline = EnhancedDataPipeline(real_data_config)\n",
    "    loader = enhanced_pipeline._get_data_loader()\n",
    "    \n",
    "    print(\"âš¡ å°è¯•åŠ è½½çœŸå®SWATæ•°æ®...\")\n",
    "    X, y, metadata = loader.load_data()\n",
    "    \n",
    "    print(f\"\\nğŸ“Š æ•°æ®åŠ è½½ç»“æœ:\")\n",
    "    print(f\"   - æ•°æ®é›†: {metadata.dataset_name}\")\n",
    "    print(f\"   - æ•°æ®å½¢çŠ¶: {X.shape}\")\n",
    "    print(f\"   - æ ‡ç­¾åˆ†å¸ƒ: æ­£å¸¸={np.sum(y==0)}, å¼‚å¸¸={np.sum(y==1)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  çœŸå®æ•°æ®ä¸å¯ç”¨: {str(e)}\")\n",
    "    print(\"ğŸ’¡ å½“å‰ä½¿ç”¨çš„æ˜¯æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º\")\n",
    "    print(\"   å¦‚éœ€ä½¿ç”¨çœŸå®æ•°æ®ï¼Œè¯·æŒ‰ä¸Šè¿°è¯´æ˜ä¸‹è½½SWATæ•°æ®é›†\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_py311_rocm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
